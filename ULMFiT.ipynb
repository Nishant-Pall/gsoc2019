{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULMFiT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNESe6v0p2II",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "\n",
        "* Overview\n",
        "\n",
        "* Pre-training the language model on Wikitext103 Dataset\n",
        "\n",
        "  1. Preprocess Wikitext 103\n",
        "  2. Make the encoder decoder architecture\n",
        "  3. Add the custom layers for AWD LSTM\n",
        "  4. Add an optimizer\n",
        "  5. Make the training loop\n",
        "  6. Calculate Perplexity\n",
        "* Fine-tuning it on new data\n",
        "   1. Preprocess the new data\n",
        "   2. train the previously saved model again with varying learning rates\n",
        "* Use the language model for classification\n",
        "   1. take the encoder for the language model, add a classifier head on top of it and use it for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAEjoUWDg-M",
        "colab_type": "code",
        "outputId": "971e9cfa-a79e-479a-9106-d0001e116e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "!pip install tensorflow-text"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c7/50d7bb8f66212a63180cfb48f0dfb1c51dd4e8f7e2b48e96b75d7f61e164/tensorflow_text-1.0.0b2-cp36-cp36m-manylinux1_x86_64.whl (6.2MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2MB 2.6MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.1,>=2.0.0b1 (from tensorflow-text)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.12.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.33.4)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.14.0a20190603)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (2.8.0)\n",
            "Installing collected packages: tensorflow, tensorflow-text\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tensorflow-2.0.0b1 tensorflow-text-1.0.0b2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESJu2YyRrgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import re\n",
        "import html "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLuYPRavnEiO",
        "colab_type": "text"
      },
      "source": [
        "# Get Data for Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RFuj9nfp0mf",
        "colab_type": "code",
        "outputId": "b2d75b64-e1c2-4864-a589-2fd87973e847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Get Wikitext 103\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-07 12:58:56--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.96.149\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.96.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190229076 (181M) [application/zip]\n",
            "Saving to: ‘wikitext-103-v1.zip’\n",
            "\n",
            "wikitext-103-v1.zip 100%[===================>] 181.42M   100MB/s    in 1.8s    \n",
            "\n",
            "2019-08-07 12:58:58 (100 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2AYGUOisTo",
        "colab_type": "code",
        "outputId": "49363ef1-75fc-4332-d12d-9f3d671b558e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip wikitext-103-v1.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wikitext-103-v1.zip\n",
            "   creating: wikitext-103/\n",
            "  inflating: wikitext-103/wiki.test.tokens  \n",
            "  inflating: wikitext-103/wiki.valid.tokens  \n",
            "  inflating: wikitext-103/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i03-og9nHXu",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess Data\n",
        "\n",
        "Two steps : \n",
        "\n",
        "1. Apply a list of rules to text \n",
        "2. Then tokenize the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUwgR3gnJ48q",
        "colab_type": "text"
      },
      "source": [
        "Note : Fastai applies these following rules before tokenization\n",
        "\n",
        "defaults.text_pre_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces]\n",
        "\n",
        "and these rules after tokenization. \n",
        "\n",
        "defaults.text_post_rules = [replace_all_caps, deal_caps]\n",
        "\n",
        "We have implementations for all the pre_rules in preprocessing function that is applied before tokenization. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjthAg6OHYef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
        "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaU5qAAgkV6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x):\n",
        "  x = x.strip().lower()\n",
        "  \n",
        "  def replace_rep(t):\n",
        "    \"Replace repetitions at the character level in text with the specified token\"\n",
        "    def _replace_rep(m):\n",
        "        c,cc = m.groups()\n",
        "        return f' {TK_REP} {len(cc)+1} {c} '\n",
        "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
        "    return re_rep.sub(_replace_rep, t)\n",
        "  \n",
        "  # replace all the characters that occur more than 3 times with the xxrep token\n",
        "  x = replace_rep(x)\n",
        "  \n",
        "  def replace_wrep(t):\n",
        "    \"Replace word repetitions in text with the specified token.\"\n",
        "    def _replace_wrep(m):\n",
        "        c,cc = m.groups()\n",
        "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
        "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
        "    return re_wrep.sub(_replace_wrep, t)\n",
        "  \n",
        "  # replaces all the words that occur more than 3 times in text gets replaced by xxwrep token\n",
        "  x = replace_wrep(x)\n",
        "  # fix html \n",
        "  re1 = re.compile(r'  +')\n",
        "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
        "  x=re1.sub(' ', html.unescape(x))\n",
        "  \n",
        "  \"Add spaces around / and # in `t`. \\n\" \n",
        "  x=re.sub(r'([/#\\n])', r' \\1 ', x)\n",
        "  \n",
        "  \"Remove multiple spaces in `t`.\"\n",
        "  \n",
        "  x=re.sub(' {2,}', ' ', x)\n",
        "  \n",
        "  \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdOiaDuwK7jv",
        "colab_type": "text"
      },
      "source": [
        "# Create Tensorflow Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2HC_P-edJDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"wikitext-103/wiki.train.tokens\"\n",
        "valid_path = \"wikitext-103/wiki.valid.tokens\"\n",
        "test_path = \"wikitext-103/wiki.test.tokens\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvhPNS5WZT5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text,target_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd-bRRdDbnhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_data(path,num_words=None):\n",
        "  data = open(path,'r').read()[0:10000]\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
        "  tokenizer.fit_on_texts([data])\n",
        "  data = tokenizer.texts_to_sequences([data])[0]\n",
        "  return tokenizer,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOC3fGmFT_Le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word_to_index = tokenizer.word_index\n",
        "#index_to_word = {v:k for k,v in tokenizer.word_index.items()}\n",
        "\n",
        "#def convert_to_text(line):\n",
        "#  return ' '.join([index_to_word[i] for i in line])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YB2M9meYwIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_lm_dataset(data,seq_length,batch_size,buffer_size):\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "  batch_set = dataset.batch(seq_length+1,drop_remainder = True)\n",
        "  batch_set = batch_set.map(lambda x:split_input_target(x))\n",
        "  return batch_set.shuffle(buffer_size).batch(batch_size,drop_remainder=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek3XeUiEP-0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer, tokenized_data = tokenize_data(train_path)\n",
        "dataset = make_lm_dataset(tokenized_data,seq_length=70,batch_size=64,buffer_size=10000)  # each batch has 64 lines, each line has 70 words"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNESe6v0p2II"
   },
   "source": [
    "# Outline\n",
    "\n",
    "* Overview\n",
    "\n",
    "* Pre-training the language model on Wikitext103 Dataset\n",
    "\n",
    "  1. Preprocess Wikitext 103\n",
    "  2. Make the encoder decoder architecture\n",
    "  3. Add the custom layers for AWD LSTM.\n",
    "  4. Add an optimizer\n",
    "  5. Make the training loop\n",
    "  6. Calculate Validation Accuracy\n",
    "* Fine-tuning it on new data\n",
    "   1. Preprocess the new data\n",
    "   2. train the previously saved model again with varying learning rates\n",
    "* Use the language model for classification\n",
    "   1. take the encoder for the language model, add a classifier head on top of it and use it for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "YhAEjoUWDg-M",
    "outputId": "4c618647-3778-4612-9c3c-b3848a7ed8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 348.9MB 83kB/s \n",
      "\u001b[K     |████████████████████████████████| 501kB 45.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.1MB 34.5MB/s \n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c7/50d7bb8f66212a63180cfb48f0dfb1c51dd4e8f7e2b48e96b75d7f61e164/tensorflow_text-1.0.0b2-cp36-cp36m-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2MB 1.4MB/s \n",
      "\u001b[?25hCollecting tensorflow<2.1,>=2.0.0b1 (from tensorflow-text)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
      "\u001b[K     |████████████████████████████████| 87.9MB 401kB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (3.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.16.4)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.14.0a20190603)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.1.7)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.33.4)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.1,>=2.0.0b1->tensorflow-text) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (41.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (2.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow<2.1,>=2.0.0b1->tensorflow-text) (3.1.1)\n",
      "Installing collected packages: tensorflow, tensorflow-text\n",
      "  Found existing installation: tensorflow 1.14.0\n",
      "    Uninstalling tensorflow-1.14.0:\n",
      "      Successfully uninstalled tensorflow-1.14.0\n",
      "Successfully installed tensorflow-2.0.0b1 tensorflow-text-1.0.0b2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UESJu2YyRrgW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLuYPRavnEiO"
   },
   "source": [
    "# Get Data for Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3RFuj9nfp0mf",
    "outputId": "16134dfe-3616-413d-efa8-ee778633d98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-11 21:07:04--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.77\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 190229076 (181M) [application/zip]\n",
      "Saving to: ‘wikitext-103-v1.zip’\n",
      "\n",
      "wikitext-103-v1.zip 100%[===================>] 181.42M  64.7MB/s    in 2.8s    \n",
      "\n",
      "2019-08-11 21:07:07 (64.7 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Wikitext 103\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "f-2AYGUOisTo",
    "outputId": "42717764-15e0-473e-ef8c-3021298fb428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-103-v1.zip\n",
      "   creating: wikitext-103/\n",
      "  inflating: wikitext-103/wiki.test.tokens  \n",
      "  inflating: wikitext-103/wiki.valid.tokens  \n",
      "  inflating: wikitext-103/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!unzip wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1i03-og9nHXu"
   },
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Two steps : \n",
    "\n",
    "1. Apply a list of rules to text \n",
    "2. Then tokenize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUwgR3gnJ48q"
   },
   "source": [
    "Note : Fastai applies these following rules before tokenization\n",
    "\n",
    "defaults.text_pre_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces]\n",
    "\n",
    "and these rules after tokenization. \n",
    "\n",
    "defaults.text_post_rules = [replace_all_caps, deal_caps]\n",
    "\n",
    "We have implementations for all the pre_rules in preprocessing function that is applied before tokenization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjthAg6OHYef"
   },
   "outputs": [],
   "source": [
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaU5qAAgkV6Q"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  x = x.strip().lower()\n",
    "  \n",
    "  def replace_rep(t):\n",
    "    \"Replace repetitions at the character level in text with the specified token\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "  \n",
    "  # replace all the characters that occur more than 3 times with the xxrep token\n",
    "  x = replace_rep(x)\n",
    "  \n",
    "  def replace_wrep(t):\n",
    "    \"Replace word repetitions in text with the specified token.\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "  \n",
    "  # replaces all the words that occur more than 3 times in text gets replaced by xxwrep token\n",
    "  x = replace_wrep(x)\n",
    "  # fix html \n",
    "  re1 = re.compile(r'  +')\n",
    "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
    "  x=re1.sub(' ', html.unescape(x))\n",
    "  \n",
    "  \"Add spaces around / and # in `t`. \\n\" \n",
    "  x=re.sub(r'([/#\\n])', r' \\1 ', x)\n",
    "  \n",
    "  \"Remove multiple spaces in `t`.\"\n",
    "  \n",
    "  x=re.sub(' {2,}', ' ', x)\n",
    "  \n",
    "  \n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdOiaDuwK7jv"
   },
   "source": [
    "# Create Tensorflow Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2HC_P-edJDQ"
   },
   "outputs": [],
   "source": [
    "train_path = \"wikitext-103/wiki.train.tokens\"\n",
    "valid_path = \"wikitext-103/wiki.valid.tokens\"\n",
    "test_path = \"wikitext-103/wiki.test.tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvhPNS5WZT5G"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text,target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vd-bRRdDbnhJ"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(path,num_words=None):\n",
    "  data = open(path,'r').read()[0:100000]\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='unk',num_words=num_words)\n",
    "  tokenizer.fit_on_texts([data])\n",
    "  data = tokenizer.texts_to_sequences([data])[0]\n",
    "  return tokenizer,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOC3fGmFT_Le"
   },
   "outputs": [],
   "source": [
    "#word_to_index = tokenizer.word_index\n",
    "#index_to_word = {v:k for k,v in tokenizer.word_index.items()}\n",
    "\n",
    "#def convert_to_text(line):\n",
    "#  return ' '.join([index_to_word[i] for i in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YB2M9meYwIi"
   },
   "outputs": [],
   "source": [
    "def make_lm_dataset(data,seq_length,batch_size,buffer_size):\n",
    "  \n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "  batch_set = dataset.batch(seq_length+1,drop_remainder = True)\n",
    "  batch_set = batch_set.map(lambda x:split_input_target(x))\n",
    "  batch_set = batch_set.shuffle(buffer_size)\n",
    "  train_dataset = batch_set.skip(50).batch(batch_size,drop_remainder=True)  \n",
    "  valid_dataset = batch_set.take(50).batch(batch_size,drop_remainder=True)\n",
    "  return train_dataset,valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ek3XeUiEP-0b"
   },
   "outputs": [],
   "source": [
    "tokenizer, tokenized_data = tokenize_data(train_path)\n",
    "train_dataset, valid_dataset= make_lm_dataset(tokenized_data,seq_length=70,batch_size=10,buffer_size=10000)  # each batch will have 64 lines, each line has 70 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLOnfxm3E5Gu"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ut_lgWGZm3WM"
   },
   "source": [
    "# Building the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xif8DnQoQkjp"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKz_Bc7bMjQv"
   },
   "outputs": [],
   "source": [
    "class EncoderModel(Model):\n",
    "  def __init__(self,vocab_size,emb_dim,seq_length,hid_dim):\n",
    "    super(EncoderModel,self).__init__()\n",
    "    self.embed = Embedding(vocab_size, emb_dim, input_length=seq_length)\n",
    "    self.lstm1 = LSTM(hid_dim,return_sequences=True)\n",
    "    self.lstm2 = LSTM(hid_dim,return_sequences=True)\n",
    "    self.lstm3 = LSTM(hid_dim,return_sequences=True)\n",
    " \n",
    "    \n",
    "  def call(self,x):\n",
    "    x = self.embed(x)\n",
    "    x = self.lstm1(x)\n",
    "    x = self.lstm2(x)\n",
    "    x = self.lstm3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQp_8xaSP_3O"
   },
   "outputs": [],
   "source": [
    "class DecoderModel(Model):\n",
    "  def __init__(self,vocab_size):\n",
    "    super(DecoderModel,self).__init__()\n",
    "    self.d2 = Dense(vocab_size,activation=\"softmax\")\n",
    "    \n",
    "  def call(self,x):\n",
    "    x = self.d2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7CMAxIDNHz9"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LveQbs4VEdD4"
   },
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self,vocab_size,emb_dim,seq_length,hid_dim):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.encoder = EncoderModel(vocab_size,emb_dim,seq_length,hid_dim)\n",
    "    self.decoder = DecoderModel(vocab_size)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nr6SXrwJWMMq"
   },
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size=vocab_size,emb_dim=64,seq_length=70,hid_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWV8-__cTkfw"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4kaqCSiElSS"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhFB0CxwFbLD"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCLjSweUFcN0"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1RIYPyaMXio"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "MyMRYiNWFez0",
    "outputId": "71565315-9189-4f2b-debb-6d29f6942e2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 21:18:49.992311 139838467336064 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.949592113494873, Accuracy: 7.915966033935547, Test Loss: 7.315667629241943, Test Accuracy: 9.028571128845215\n",
      "Epoch 2, Loss: 6.855672836303711, Accuracy: 8.369747161865234, Test Loss: 6.98321008682251, Test Accuracy: 9.028571128845215\n",
      "Epoch 3, Loss: 6.654736042022705, Accuracy: 8.369747161865234, Test Loss: 7.094948768615723, Test Accuracy: 9.028571128845215\n",
      "Epoch 4, Loss: 6.624736785888672, Accuracy: 8.369747161865234, Test Loss: 7.151017665863037, Test Accuracy: 9.028571128845215\n",
      "Epoch 5, Loss: 6.611464500427246, Accuracy: 8.369747161865234, Test Loss: 7.183828830718994, Test Accuracy: 9.028571128845215\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for source_sent, target_sent in train_dataset:\n",
    "    train_step(source_sent, target_sent)\n",
    "    \n",
    "  for source_sent, target_sent in valid_dataset:\n",
    "    test_step(source_sent,target_sent)\n",
    "    \n",
    "    \n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "  # Reset the metrics for the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7qxRlp4jO9w"
   },
   "source": [
    "# Using Language Model as Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dOz7pXV8XozM",
    "outputId": "266a1f71-2d19-4c45-91df-fcd82a4a5b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TJDfNHyjToz"
   },
   "outputs": [],
   "source": [
    "my_enc = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5u6gdd_Sjbjp"
   },
   "outputs": [],
   "source": [
    "classifier = "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1i03-og9nHXu"
   ],
   "name": "ULMFiT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULMFiT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "MLuYPRavnEiO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNESe6v0p2II",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "\n",
        "* Overview\n",
        "\n",
        "* Pre-training the language model on Wikitext103 Dataset\n",
        "\n",
        "  1. Preprocess Wikitext 103\n",
        "  2. Make the encoder decoder architecture\n",
        "  3. Add the custom layers for AWD LSTM\n",
        "  4. Add an optimizer\n",
        "  5. Make the training loop\n",
        "  6. Calculate Perplexity\n",
        "* Fine-tuning it on new data\n",
        "   1. Preprocess the new data\n",
        "   2. train the previously saved model again with varying learning rates\n",
        "* Use the language model for classification\n",
        "   1. take the encoder for the language model, add a classifier head on top of it and use it for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAEjoUWDg-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "187c6db3-6273-494d-f98f-cf654feaf07b"
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 37.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 40.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESJu2YyRrgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import re\n",
        "import html "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLuYPRavnEiO",
        "colab_type": "text"
      },
      "source": [
        "# Get Data for Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RFuj9nfp0mf",
        "colab_type": "code",
        "outputId": "a2502ce4-d99e-4f2a-9aab-3ba322221b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Get Wikitext 103\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-05 06:50:35--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.239.13\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.239.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190229076 (181M) [application/zip]\n",
            "Saving to: ‘wikitext-103-v1.zip’\n",
            "\n",
            "wikitext-103-v1.zip 100%[===================>] 181.42M  36.6MB/s    in 5.4s    \n",
            "\n",
            "2019-08-05 06:50:41 (33.4 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2AYGUOisTo",
        "colab_type": "code",
        "outputId": "466da7bf-264e-4a40-c1be-c937c91fb305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip wikitext-103-v1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wikitext-103-v1.zip\n",
            "   creating: wikitext-103/\n",
            "  inflating: wikitext-103/wiki.test.tokens  \n",
            "  inflating: wikitext-103/wiki.valid.tokens  \n",
            "  inflating: wikitext-103/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i03-og9nHXu",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess Data\n",
        "\n",
        "Two steps : \n",
        "\n",
        "1. Apply a list of rules to text \n",
        "2. Then tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG48LNBGi4o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"wikitext-103/wiki.train.tokens\"\n",
        "valid_path = \"wikitext-103/wiki.valid.tokens\"\n",
        "test_path = \"wikitext-103/wiki.test.tokens\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpj2srUwjChP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "2a95215b-339d-4d37-c25f-ab01adbefae5"
      },
      "source": [
        "data = open(train_path,\"r\").readlines()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2333cd577305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikitext-103/wiki.train.tokens'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUwgR3gnJ48q",
        "colab_type": "text"
      },
      "source": [
        "Note : Fastai applies these following rules before tokenization\n",
        "\n",
        "defaults.text_pre_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces]\n",
        "\n",
        "and these rules after tokenization. \n",
        "\n",
        "defaults.text_post_rules = [replace_all_caps, deal_caps]\n",
        "\n",
        "We have implementations for all the pre_rules in preprocessing function that is applied before tokenization. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjthAg6OHYef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
        "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaU5qAAgkV6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x):\n",
        "  x = x.strip().lower()\n",
        "  \n",
        "  def replace_rep(t):\n",
        "    \"Replace repetitions at the character level in text with the specified token\"\n",
        "    def _replace_rep(m):\n",
        "        c,cc = m.groups()\n",
        "        return f' {TK_REP} {len(cc)+1} {c} '\n",
        "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
        "    return re_rep.sub(_replace_rep, t)\n",
        "  \n",
        "  # replace all the characters that occur more than 3 times with the xxrep token\n",
        "  x = replace_rep(x)\n",
        "  \n",
        "  def replace_wrep(t):\n",
        "    \"Replace word repetitions in text with the specified token.\"\n",
        "    def _replace_wrep(m):\n",
        "        c,cc = m.groups()\n",
        "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
        "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
        "    return re_wrep.sub(_replace_wrep, t)\n",
        "  \n",
        "  # replaces all the words that occur more than 3 times in text gets replaced by xxwrep token\n",
        "  x = replace_wrep(x)\n",
        "  # fix html \n",
        "  re1 = re.compile(r'  +')\n",
        "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
        "  x=re1.sub(' ', html.unescape(x))\n",
        "  \n",
        "  \"Add spaces around / and # in `t`. \\n\" \n",
        "  x=re.sub(r'([/#\\n])', r' \\1 ', x)\n",
        "  \n",
        "  \"Remove multiple spaces in `t`.\"\n",
        "  \n",
        "  x=re.sub(' {2,}', ' ', x)\n",
        "  \n",
        "  \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTE9aQTeTSuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(data,**kwargs):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(**kwargs)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  return tokenizer,tokenizer.texts_to_sequences(data)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHFtZEm2IFtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [preprocess(x) for x in data[0:500]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpbbbZO0V3Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer, result = tokenize(data,oov_token='xxunk',num_words=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdOiaDuwK7jv",
        "colab_type": "text"
      },
      "source": [
        "# Create Tensorflow Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkLRCWhRSS2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
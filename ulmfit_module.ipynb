{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM,Input,Embedding\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.int64)])\n",
    "class LanguageModelEncoder(Model):\n",
    "    def __init__(self,vocab_size,emb_dim,state_size,n_layers):\n",
    "        super(LanguageModelEncoder, self).__init__()\n",
    "        self._state_size = state_size\n",
    "        self.embedding_layer = Embedding(vocab_size,emb_dim)\n",
    "        self._lstm_layers = [LSTM(self._state_size,return_sequences=True) for i in range(n_layers)]\n",
    "        #self._lstm_layer = tf.keras.layers.LSTM(state_size,return_sequences=True)\n",
    "        \n",
    "    def __call__(self,sentence_lookup_ids):\n",
    "        \n",
    "        emb_output = self.embedding_layer(sentence_lookup_ids)\n",
    "        lstm_output = emb_output # initialize to the input\n",
    "        for lstm_layer in self._lstm_layers:\n",
    "            lstm_output = lstm_layer(lstm_output)\n",
    "        return lstm_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocabulary_file(vocabulary):\n",
    "  \"\"\"Write temporary vocab file for module construction.\"\"\"\n",
    "  tmpdir = tempfile.mkdtemp()\n",
    "  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n",
    "  with tf.io.gfile.GFile(vocabulary_file, \"w\") as f:\n",
    "    for entry in vocabulary:\n",
    "      f.write(entry + \"\\n\")\n",
    "  return vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiTModule(tf.train.Checkpoint):\n",
    "  \"\"\"\n",
    "  LATER \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab, emb_dim, buckets, state_size,n_layers):\n",
    "    super(ULMFiTModule, self).__init__()\n",
    "    self._buckets = buckets\n",
    "    self._vocab_size = len(vocab)\n",
    "    self.emb_row_size = self._vocab_size+self._buckets\n",
    "    #self._embeddings = tf.Variable(tf.random.uniform(shape=[self.emb_row_size, emb_dim]))\n",
    "    self._state_size = state_size\n",
    "    self.model = LanguageModelEncoder(self.emb_row_size,emb_dim,state_size,n_layers)\n",
    "    self._vocabulary_file = tracking.TrackableAsset(write_vocabulary_file(vocab)) \n",
    "    self.w2i_table = lookup_ops.index_table_from_file(\n",
    "                    vocabulary_file= self._vocabulary_file,\n",
    "                    num_oov_buckets=self._buckets,\n",
    "                    hasher_spec=lookup_ops.FastHashSpec)\n",
    "    self.i2w_table = lookup_ops.index_to_string_table_from_file(\n",
    "                    vocabulary_file=self._vocabulary_file, \n",
    "                    delimiter = '\\n',\n",
    "                    default_value=\"UNKNOWN\")\n",
    "    self._logit_layer = tf.keras.layers.Dense(self.emb_row_size)\n",
    "\n",
    "\n",
    "    \n",
    "  def _tokenize(self, sentences):\n",
    "    # Perform a minimalistic text preprocessing by removing punctuation and\n",
    "    # splitting on spaces.\n",
    "    normalized_sentences = tf.strings.regex_replace(\n",
    "        input=sentences, pattern=r\"\\pP\", rewrite=\"\")\n",
    "    sparse_tokens = tf.strings.split(normalized_sentences, \" \").to_sparse()\n",
    "\n",
    "    # Deal with a corner case: there is one empty sentence.\n",
    "    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(\"\"))\n",
    "    # Deal with a corner case: all sentences are empty.\n",
    "    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n",
    "\n",
    "    return (sparse_tokens.indices, sparse_tokens.values,\n",
    "            sparse_tokens.dense_shape)\n",
    "    \n",
    "  def _indices_to_words(self, indices):\n",
    "    #return tf.gather(self._vocab_tensor, indices)\n",
    "    return self.i2w_table.lookup(indices)\n",
    "    \n",
    "\n",
    "  def _words_to_indices(self, words):\n",
    "    #return tf.strings.to_hash_bucket(words, self._buckets)\n",
    "    return self.w2i_table.lookup(words)\n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None],tf.dtypes.string)])   \n",
    "  def _tokens_to_lookup_ids(self,sentences):\n",
    "    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n",
    "    tokens_sparse = tf.sparse.SparseTensor(\n",
    "        indices=token_ids, values=token_values, dense_shape=token_dense_shape)\n",
    "    tokens = tf.sparse.to_dense(tokens_sparse, default_value=\"\")\n",
    "\n",
    "    sparse_lookup_ids = tf.sparse.SparseTensor(\n",
    "        indices=tokens_sparse.indices,\n",
    "        values=self._words_to_indices(tokens_sparse.values),\n",
    "        dense_shape=tokens_sparse.dense_shape)\n",
    "    lookup_ids = tf.sparse.to_dense(sparse_lookup_ids, default_value=0)\n",
    "    return tokens,lookup_ids\n",
    "        \n",
    "    \n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n",
    "  def train(self, sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "      #sentence_embeddings = tf.nn.embedding_lookup(self._embeddings,tokens_ids_seq)\n",
    "    \n",
    "      lstm_output = self.model(tokens_ids_seq)\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "      targets = tf.reshape(tokens_ids_target, [-1])\n",
    "      weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "      # Final loss is the mean loss for all token losses.\n",
    "      final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_loss\")\n",
    "\n",
    "    watched = t.watched_variables()\n",
    "    gradients = t.gradient(final_loss, watched)\n",
    "\n",
    "    for w, g in zip(watched, gradients):\n",
    "      w.assign_sub(g)\n",
    "\n",
    "    return final_loss\n",
    "  @tf.function\n",
    "  def return_encoder(self):\n",
    "    return self._model\n",
    "    \n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])  \n",
    "  def validate(self,sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    lstm_output = self.model(tokens_ids_seq)\n",
    "    lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "    logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "    targets = tf.reshape(tokens_ids_target, [-1])\n",
    "    weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "    # Final loss is the mean loss for all token losses.\n",
    "    final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_validation_loss\")\n",
    "\n",
    "    return final_loss\n",
    "    \n",
    "  @tf.function\n",
    "  def decode_greedy(self, sequence_length, first_word):\n",
    "    #initial_state = self._lstm_cell.get_initial_state(\n",
    "    #    dtype=tf.float32, batch_size=1)\n",
    "\n",
    "    sequence = [first_word]\n",
    "    current_word = first_word\n",
    "    current_id = tf.expand_dims(self._words_to_indices(current_word), 0)\n",
    "    #current_state = initial_state\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "      #token_embeddings = tf.nn.embedding_lookup(self._embeddings, current_id)\n",
    "      #token_embeddings = tf.expand_dims(token_embeddings,0)\n",
    "      #logits = self.model(tf.expand_dims(token_embeddings,0))\n",
    "      lstm_output = self.model(tf.expand_dims(current_id,0))\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      softmax = tf.nn.softmax(logits)\n",
    "\n",
    "      next_ids = tf.math.argmax(softmax, axis=1)\n",
    "      next_words = self._indices_to_words(next_ids)[0]\n",
    "      \n",
    "      current_id = next_ids\n",
    "      current_word = next_words\n",
    "      sequence.append(current_word)\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n\n    TypeError: __init__() missing 3 required positional arguments: 'emb_dim', 'state_size', and 'n_layers'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-680ae946bfa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m   ]\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mULMFiTModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ba70ead03926>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab, emb_dim, buckets, state_size, n_layers)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#self._embeddings = tf.Variable(tf.random.uniform(shape=[self.emb_row_size, emb_dim]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLanguageModelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_row_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vocabulary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrackableAsset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_vocabulary_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     self.w2i_table = lookup_ops.index_table_from_file(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    357\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    358\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 359\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1646\u001b[0m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1648\u001b[1;33m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1649\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1539\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1541\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   1542\u001b[0m         self._function_attributes)\n\u001b[0;32m   1543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    714\u001b[0m                                           converted_func)\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    704\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n\n    TypeError: __init__() missing 3 required positional arguments: 'emb_dim', 'state_size', and 'n_layers'\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\",\n",
    "             \"<S> hello world <E>\", \"<S> who are you? <E>\"]\n",
    "validation_sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"fine\",\"thank\",\"world\",\n",
    "    \"who\"\n",
    "  ]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=1, state_size=128,n_layers=1)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = module.train(tf.constant(sentences))\n",
    "    validation_loss = module.validate(tf.constant(validation_sentences))\n",
    "    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy(),\" Validation loss \",validation_loss.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> you', b'are', b'you', b'there', b'<E>', b'are', b'you', b'there', b'<E>', b'are', b'you']\n"
     ]
    }
   ],
   "source": [
    " # We have to call this function explicitly if we want it exported, because it\n",
    "  # has no input_signature in the @tf.function decorator.\n",
    "decoded = module.decode_greedy(sequence_length=10, first_word=tf.constant(\"<S> you\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 20:12:06.196258  3736 saved_model.py:758] Skipping full serialization of Keras model <__main__.LanguageModelEncoder object at 0x00000229E6ADA080>, because its inputs are not defined.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(module,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module = tf.saved_model.load(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = b.decode_greedy(sequence_length=10,first_word=tf.constant(\"<S> Hello\"))\n",
    "#_ = [d.numpy() for d in decoded]\n",
    "#print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Head \n",
    "\n",
    "\n",
    "Classifier head takes in the final layer output of the languaage model and first gets the average pool and max pool of the \n",
    "final layer outputs, then passes the concatanation of last time steps hidden state, max pool results and average pool results through given number Dense-dropout-batchnormalization blocks. Finally it produces the classifier output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(Model):\n",
    "    def __init__(self,language_module,num_labels,dense_units=(128,128),dropouts=(0.1,0.1)):\n",
    "        \n",
    "        # initialization stuff\n",
    "        super(LanguageClassifier,self).__init__()\n",
    "        self._language_module = language_module\n",
    "        self.model_encoder = language_module.model\n",
    "        \n",
    "        \n",
    "        # classifier head layers\n",
    "        self.dense_layers = [Dense(units,activation=\"relu\") for units in dense_units]\n",
    "        self.dropout_layers = [Dropout(p) for p in dropouts]\n",
    "        self.max_pool_layer = GlobalMaxPooling1D()\n",
    "        self.average_pool_layer = GlobalAveragePooling1D()\n",
    "        self.batchnorm_layer = BatchNormalization()\n",
    "        self.n_layers = len(self.dense_layers)\n",
    "        self.final_layer = Dense(num_labels,activation=\"sigmoid\")\n",
    "        \n",
    "    def __call__(self,sentences):\n",
    "        \n",
    "        tokens,lookup_ids = self._language_module._tokens_to_lookup_ids(sentences)\n",
    "        print(lookup_ids.dtype)\n",
    "        self.enc_out = self.model_encoder(lookup_ids)\n",
    "        last_h = self.enc_out[:,-1,:]\n",
    "        max_pool_output = self.max_pool_layer(self.enc_out)\n",
    "        average_pool_output = self.average_pool_layer(self.enc_out)\n",
    "        \n",
    "        output = concatenate([last_h,max_pool_output,average_pool_output])\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = self.dense_layers[i](output)\n",
    "            output = self.dropout_layers[i](output)\n",
    "            output = self.batchnorm_layer(output)\n",
    "        \n",
    "        final_output = self.final_layer(output)\n",
    "        return final_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageClassifier(num_labels=2,language_module=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.LanguageModelEncoder at 0x2598f812a20>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x259971a5e48>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x259971a5278>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x2599b84ea20>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x2599b4efa20>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D at 0x2599b4efe80>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D at 0x259971a5cc0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2599b4ef400>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2599b4ef358>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "probabilities = model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=11539, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.51236403, 0.39347237],\n",
       "       [0.56130195, 0.3755333 ],\n",
       "       [0.5554221 , 0.41424546],\n",
       "       [0.5123186 , 0.39361775],\n",
       "       [0.5151545 , 0.4016927 ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6686174273490906, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 2, Loss: 0.6479798555374146, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 3, Loss: 0.6051146388053894, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 4, Loss: 0.5851243734359741, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 5, Loss: 0.5470303297042847, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "labels = tf.constant([[1],[0],[1],[0],[0]])\n",
    "@tf.function\n",
    "def train_step(samples, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(samples)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_step(sentences, labels)\n",
    "    #test_step(validation_sentences, validation_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "  # Reset the metrics for the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (probabilities.numpy() > 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

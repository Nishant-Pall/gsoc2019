{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM,Input\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(Model):\n",
    "    def __init__(self,state_size,logit_units):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self._lstm_layer = tf.keras.layers.LSTM(state_size,return_sequences=True)\n",
    "        self._logit_layer = tf.keras.layers.Dense(logit_units)\n",
    "        \n",
    "    def __call__(self,sentence_embeddings):\n",
    "        lstm_output = self._lstm_layer(sentence_embeddings)\n",
    "        lstm_output = tf.reshape(lstm_output, [-1,self._lstm_layer.units])\n",
    "        logits = self._logit_layer(lstm_output)\n",
    "        return logits\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocabulary_file(vocabulary):\n",
    "  \"\"\"Write temporary vocab file for module construction.\"\"\"\n",
    "  tmpdir = tempfile.mkdtemp()\n",
    "  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n",
    "  with tf.io.gfile.GFile(vocabulary_file, \"w\") as f:\n",
    "    for entry in vocabulary:\n",
    "      f.write(entry + \"\\n\")\n",
    "  return vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiTModule(tf.train.Checkpoint):\n",
    "  \"\"\"\n",
    "  LATER \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab, emb_dim, buckets, state_size):\n",
    "    super(ULMFiTModule, self).__init__()\n",
    "    self._buckets = buckets\n",
    "    self._vocab_size = len(vocab)\n",
    "    self.emb_row_size = self._vocab_size+self._buckets\n",
    "    self._embeddings = tf.Variable(tf.random.uniform(shape=[self.emb_row_size, emb_dim]))\n",
    "    print(self._embeddings.shape)\n",
    "    self.model = LanguageModel(state_size,self.emb_row_size)\n",
    "    self._vocabulary_file = tracking.TrackableAsset(write_vocabulary_file(vocab)) \n",
    "    self.w2i_table = lookup_ops.index_table_from_file(\n",
    "                    vocabulary_file= self._vocabulary_file,\n",
    "                    num_oov_buckets=self._buckets,\n",
    "                    hasher_spec=lookup_ops.FastHashSpec)\n",
    "    self.i2w_table = lookup_ops.index_to_string_table_from_file(\n",
    "                    vocabulary_file=self._vocabulary_file, \n",
    "                    delimiter = '\\n',\n",
    "                    default_value=\"UNKNOWN\")\n",
    "\n",
    "    \n",
    "  def _tokenize(self, sentences):\n",
    "    # Perform a minimalistic text preprocessing by removing punctuation and\n",
    "    # splitting on spaces.\n",
    "    normalized_sentences = tf.strings.regex_replace(\n",
    "        input=sentences, pattern=r\"\\pP\", rewrite=\"\")\n",
    "    sparse_tokens = tf.strings.split(normalized_sentences, \" \").to_sparse()\n",
    "\n",
    "    # Deal with a corner case: there is one empty sentence.\n",
    "    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(\"\"))\n",
    "    # Deal with a corner case: all sentences are empty.\n",
    "    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n",
    "\n",
    "    return (sparse_tokens.indices, sparse_tokens.values,\n",
    "            sparse_tokens.dense_shape)\n",
    "    \n",
    "  def _indices_to_words(self, indices):\n",
    "    #return tf.gather(self._vocab_tensor, indices)\n",
    "    return self.i2w_table.lookup(indices)\n",
    "    \n",
    "\n",
    "  def _words_to_indices(self, words):\n",
    "    #return tf.strings.to_hash_bucket(words, self._buckets)\n",
    "    return self.w2i_table.lookup(words)\n",
    "    \n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string),tf.TensorSpec([None], tf.dtypes.string)])\n",
    "  def train(self, sentences,validation_sentences=None):\n",
    "    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n",
    "    tokens_sparse = tf.sparse.SparseTensor(\n",
    "        indices=token_ids, values=token_values, dense_shape=token_dense_shape)\n",
    "    tokens = tf.sparse.to_dense(tokens_sparse, default_value=\"\")\n",
    "\n",
    "    sparse_lookup_ids = tf.sparse.SparseTensor(\n",
    "        indices=tokens_sparse.indices,\n",
    "        values=self._words_to_indices(tokens_sparse.values),\n",
    "        dense_shape=tokens_sparse.dense_shape)\n",
    "    lookup_ids = tf.sparse.to_dense(sparse_lookup_ids, default_value=0)\n",
    "    \n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "      sentence_embeddings = tf.nn.embedding_lookup(self._embeddings,\n",
    "                                                   tokens_ids_seq)\n",
    "    \n",
    "      logits = self.model(sentence_embeddings)\n",
    "      \n",
    "\n",
    "      targets = tf.reshape(tokens_ids_target, [-1])\n",
    "      weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "      # Final loss is the mean loss for all token losses.\n",
    "      final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_loss\")\n",
    "\n",
    "    watched = t.watched_variables()\n",
    "    gradients = t.gradient(final_loss, watched)\n",
    "\n",
    "    for w, g in zip(watched, gradients):\n",
    "      w.assign_sub(g)\n",
    "\n",
    "    return final_loss,logits\n",
    "\n",
    "  @tf.function\n",
    "  def decode_greedy(self, sequence_length, first_word):\n",
    "    #initial_state = self._lstm_cell.get_initial_state(\n",
    "    #    dtype=tf.float32, batch_size=1)\n",
    "\n",
    "    sequence = [first_word]\n",
    "    current_word = first_word\n",
    "    current_id = tf.expand_dims(self._words_to_indices(current_word), 0)\n",
    "    #current_state = initial_state\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "      token_embeddings = tf.nn.embedding_lookup(self._embeddings, current_id)\n",
    "      \n",
    "      logits = self.model(tf.expand_dims(token_embeddings,0))\n",
    "      softmax = tf.nn.softmax(logits)\n",
    "\n",
    "      next_ids = tf.math.argmax(softmax, axis=1)\n",
    "      next_words = self._indices_to_words(next_ids)[0]\n",
    "      \n",
    "      current_id = next_ids\n",
    "      current_word = next_words\n",
    "      sequence.append(current_word)\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n",
      "(None, None, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.7970374, shape=(), dtype=float32) (30, 16)\n",
      "tf.Tensor(2.5767746, shape=(), dtype=float32) (30, 16)\n",
      "tf.Tensor(2.4554627, shape=(), dtype=float32) (30, 16)\n",
      "tf.Tensor(2.3978527, shape=(), dtype=float32) (30, 16)\n",
      "tf.Tensor(2.3649178, shape=(), dtype=float32) (30, 16)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\",\n",
    "             \"<S> hello world <E>\", \"<S> who are you? <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"fine\",\"thank\",\"world\",\n",
    "    \"who\"\n",
    "  ]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=1, state_size=128)\n",
    "\n",
    "for _ in range(5):\n",
    "    _,logits = module.train(tf.constant(sentences),tf.constant(sentences))\n",
    "    print(_,logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> you', b'are', b'you', b'there', b'<E>', b'fine', b'thank', b'you', b'there', b'<E>', b'fine']\n"
     ]
    }
   ],
   "source": [
    " # We have to call this function explicitly if we want it exported, because it\n",
    "  # has no input_signature in the @tf.function decorator.\n",
    "decoded = module.decode_greedy(sequence_length=10, first_word=tf.constant(\"<S> you\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 16:23:56.693716  4508 saved_model.py:758] Skipping full serialization of Keras model <__main__.LanguageModel object at 0x000001F0185C14A8>, because its inputs are not defined.\n",
      "W0816 16:23:56.695698  4508 saved_model.py:765] Skipping full serialization of Keras layer <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x000001F0182CD438>, because it is not built.\n",
      "W0816 16:23:56.697700  4508 saved_model.py:765] Skipping full serialization of Keras layer <tensorflow.python.keras.layers.recurrent.RNN object at 0x000001F0197AEC50>, because it is not built.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(module,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.saved_model.load(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> Hello', b'hello', b'there', b'I', b'am', b'am', b'am', b'am', b'am', b'am', b'am']\n"
     ]
    }
   ],
   "source": [
    "d = b.decode_greedy(sequence_length=10,first_word=tf.constant(\"<S> Hello\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Head \n",
    "\n",
    "\n",
    "Classifier head takes in the final layer output of the languaage model and first gets the average pool and max pool of the \n",
    "final layer outputs, then passes the concatanation of last time steps hidden state, max pool results and average pool results through given number Dense-dropout-batchnormalization blocks. Finally it produces the classifier output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(Model):\n",
    "    def __init__(self,num_labels,dense_units=(128,128),dropouts=(0.1,0.1)):\n",
    "        super(LanguageClassifier,self).__init__()\n",
    "        self.dense_layers = [Dense(units,activation=\"relu\") for units in dense_units]\n",
    "        self.dropout_layers = [Dropout(p) for p in dropouts]\n",
    "        self.max_pool_layer = GlobalMaxPooling1D()\n",
    "        self.average_pool_layer = GlobalAveragePooling1D()\n",
    "        self.batchnorm_layer = BatchNormalization()\n",
    "        self.n_layers = len(self.dense_layers)\n",
    "        self.final_layer = Dense(num_labels,activation=\"sigmoid\")\n",
    "        \n",
    "    def __call__(self,encoder_output):\n",
    "        self.enc_out = encoder_output\n",
    "        last_h = self.enc_out[:,-1,:]\n",
    "        max_pool_output = max_pool_layer(self.enc_out)\n",
    "        average_pool_output = average_pool_layer(self.enc_out)\n",
    "        \n",
    "        output = concatenate([last_h,max_pool_output,average_pool_output])\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = self.dense_layers[i](output)\n",
    "            output = self.dropout_layers[i](output)\n",
    "            output = self.batchnorm_layer(output)\n",
    "        \n",
    "        final_output = self.final_layer(output)\n",
    "        return final_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

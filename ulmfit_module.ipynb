{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(Model):\n",
    "    def __init__(self,state_size,buckets):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self._lstm_cell = tf.keras.layers.LSTMCell(units=state_size)\n",
    "        self._rnn_layer = tf.keras.layers.RNN(self._lstm_cell, return_sequences=True)\n",
    "        self._lstm_layer = tf.keras.layers.LSTM(state_size,return_sequences=True)\n",
    "        self._logit_layer = tf.keras.layers.Dense(buckets)\n",
    "        \n",
    "    def __call__(self,sentence_embeddings):\n",
    "        lstm_output = self._lstm_layer(sentence_embeddings)\n",
    "        lstm_output = tf.reshape(lstm_output, [-1,self._lstm_layer.units])\n",
    "        logits = self._logit_layer(lstm_output)\n",
    "        return logits\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocabulary_file(vocabulary):\n",
    "  \"\"\"Write temporary vocab file for module construction.\"\"\"\n",
    "  tmpdir = tempfile.mkdtemp()\n",
    "  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n",
    "  with tf.io.gfile.GFile(vocabulary_file, \"w\") as f:\n",
    "    for entry in vocabulary:\n",
    "      f.write(entry + \"\\n\")\n",
    "  return vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiTModule(tf.train.Checkpoint):\n",
    "  \"\"\"\n",
    "  LATER \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab, emb_dim, buckets, state_size,finetune=None,module_path=None):\n",
    "    super(ULMFiTModule, self).__init__()\n",
    "    self._buckets = buckets\n",
    "    self._vocab_size = len(vocab)\n",
    "    self._embeddings = tf.Variable(tf.random.uniform(shape=[self._vocab_size+self._buckets-1, emb_dim]))\n",
    "    print(self._embeddings.shape)\n",
    "    self.model = LanguageModel(state_size,buckets)\n",
    "    self._vocabulary_file = tracking.TrackableAsset(write_vocabulary_file(vocab)) \n",
    "    self.w2i_table = lookup_ops.index_table_from_file(\n",
    "                    vocabulary_file= self._vocabulary_file,\n",
    "                    num_oov_buckets=self._buckets,\n",
    "                    hasher_spec=lookup_ops.FastHashSpec)\n",
    "    self.i2w_table = lookup_ops.index_to_string_table_from_file(\n",
    "                    vocabulary_file=self._vocabulary_file, \n",
    "                    delimiter = '\\n',\n",
    "                    default_value=\"UNKNOWN\")\n",
    "    #self._set_up_vocab(vocab)\n",
    "\n",
    "  def _set_up_vocab(self,sentences):\n",
    "    pass\n",
    "\n",
    "    \n",
    "  def _tokenize(self, sentences):\n",
    "    # Perform a minimalistic text preprocessing by removing punctuation and\n",
    "    # splitting on spaces.\n",
    "    normalized_sentences = tf.strings.regex_replace(\n",
    "        input=sentences, pattern=r\"\\pP\", rewrite=\"\")\n",
    "    sparse_tokens = tf.strings.split(normalized_sentences, \" \").to_sparse()\n",
    "\n",
    "    # Deal with a corner case: there is one empty sentence.\n",
    "    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(\"\"))\n",
    "    # Deal with a corner case: all sentences are empty.\n",
    "    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n",
    "\n",
    "    return (sparse_tokens.indices, sparse_tokens.values,\n",
    "            sparse_tokens.dense_shape)\n",
    "    \n",
    "  def _indices_to_words(self, indices):\n",
    "    #return tf.gather(self._vocab_tensor, indices)\n",
    "    return self.i2w_table.lookup(indices)\n",
    "    \n",
    "\n",
    "  def _words_to_indices(self, words):\n",
    "    #return tf.strings.to_hash_bucket(words, self._buckets)\n",
    "    return self.w2i_table.lookup(words)\n",
    "    \n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n",
    "  def train(self, sentences):\n",
    "    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n",
    "    tokens_sparse = tf.sparse.SparseTensor(\n",
    "        indices=token_ids, values=token_values, dense_shape=token_dense_shape)\n",
    "    tokens = tf.sparse.to_dense(tokens_sparse, default_value=\"\")\n",
    "\n",
    "    sparse_lookup_ids = tf.sparse.SparseTensor(\n",
    "        indices=tokens_sparse.indices,\n",
    "        values=self._words_to_indices(tokens_sparse.values),\n",
    "        dense_shape=tokens_sparse.dense_shape)\n",
    "    lookup_ids = tf.sparse.to_dense(sparse_lookup_ids, default_value=0)\n",
    "    \n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "      sentence_embeddings = tf.nn.embedding_lookup(self._embeddings,\n",
    "                                                   tokens_ids_seq)\n",
    "    \n",
    "      logits = self.model(sentence_embeddings)\n",
    "\n",
    "      \n",
    "\n",
    "      targets = tf.reshape(tokens_ids_target, [-1])\n",
    "      weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "      # Final loss is the mean loss for all token losses.\n",
    "      final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_loss\")\n",
    "\n",
    "    watched = t.watched_variables()\n",
    "    gradients = t.gradient(final_loss, watched)\n",
    "\n",
    "    for w, g in zip(watched, gradients):\n",
    "      w.assign_sub(g)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "  @tf.function\n",
    "  def decode_greedy(self, sequence_length, first_word):\n",
    "    #initial_state = self._lstm_cell.get_initial_state(\n",
    "    #    dtype=tf.float32, batch_size=1)\n",
    "\n",
    "    sequence = [first_word]\n",
    "    current_word = first_word\n",
    "    current_id = tf.expand_dims(self._words_to_indices(current_word), 0)\n",
    "    #current_state = initial_state\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "      token_embeddings = tf.nn.embedding_lookup(self._embeddings, current_id)\n",
    "      \n",
    "      logits = self.model(tf.expand_dims(token_embeddings,0))\n",
    "      softmax = tf.nn.softmax(logits)\n",
    "\n",
    "      next_ids = tf.math.argmax(softmax, axis=1)\n",
    "      next_words = self._indices_to_words(next_ids)[0]\n",
    "      \n",
    "      current_id = next_ids\n",
    "      current_word = next_words\n",
    "      sequence.append(current_word)\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.613183, shape=(), dtype=float32)\n",
      "tf.Tensor(4.328571, shape=(), dtype=float32)\n",
      "tf.Tensor(4.031907, shape=(), dtype=float32)\n",
      "tf.Tensor(3.6816099, shape=(), dtype=float32)\n",
      "tf.Tensor(3.3656678, shape=(), dtype=float32)\n",
      "tf.Tensor(3.1434155, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9425628, shape=(), dtype=float32)\n",
      "tf.Tensor(2.776624, shape=(), dtype=float32)\n",
      "tf.Tensor(2.6492114, shape=(), dtype=float32)\n",
      "tf.Tensor(2.54954, shape=(), dtype=float32)\n",
      "tf.Tensor(2.471102, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4142792, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4267638, shape=(), dtype=float32)\n",
      "tf.Tensor(2.5892394, shape=(), dtype=float32)\n",
      "tf.Tensor(3.0620918, shape=(), dtype=float32)\n",
      "tf.Tensor(2.237078, shape=(), dtype=float32)\n",
      "tf.Tensor(2.1876364, shape=(), dtype=float32)\n",
      "tf.Tensor(2.1995685, shape=(), dtype=float32)\n",
      "tf.Tensor(2.3350883, shape=(), dtype=float32)\n",
      "tf.Tensor(2.783279, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0047796, shape=(), dtype=float32)\n",
      "tf.Tensor(1.9484298, shape=(), dtype=float32)\n",
      "tf.Tensor(1.9184636, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0144162, shape=(), dtype=float32)\n",
      "tf.Tensor(2.204487, shape=(), dtype=float32)\n",
      "tf.Tensor(2.70635, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8190792, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8367138, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8199228, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8592781, shape=(), dtype=float32)\n",
      "tf.Tensor(2.1681132, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5082928, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4814472, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5193255, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7250907, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4832166, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8140519, shape=(), dtype=float32)\n",
      "tf.Tensor(1.2620418, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3235476, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3413327, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5632846, shape=(), dtype=float32)\n",
      "tf.Tensor(1.1545304, shape=(), dtype=float32)\n",
      "tf.Tensor(1.280556, shape=(), dtype=float32)\n",
      "tf.Tensor(1.1669296, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3691409, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9904695, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0671291, shape=(), dtype=float32)\n",
      "tf.Tensor(0.982543, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0901849, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8733495, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9126692, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8085631, shape=(), dtype=float32)\n",
      "tf.Tensor(0.82573146, shape=(), dtype=float32)\n",
      "tf.Tensor(0.73819315, shape=(), dtype=float32)\n",
      "tf.Tensor(0.7318134, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6656322, shape=(), dtype=float32)\n",
      "tf.Tensor(0.64350885, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5947758, shape=(), dtype=float32)\n",
      "tf.Tensor(0.56734425, shape=(), dtype=float32)\n",
      "tf.Tensor(0.53202474, shape=(), dtype=float32)\n",
      "tf.Tensor(0.50717056, shape=(), dtype=float32)\n",
      "tf.Tensor(0.48202175, shape=(), dtype=float32)\n",
      "tf.Tensor(0.46250942, shape=(), dtype=float32)\n",
      "tf.Tensor(0.4447468, shape=(), dtype=float32)\n",
      "tf.Tensor(0.43022045, shape=(), dtype=float32)\n",
      "tf.Tensor(0.41741687, shape=(), dtype=float32)\n",
      "tf.Tensor(0.40632486, shape=(), dtype=float32)\n",
      "tf.Tensor(0.39636293, shape=(), dtype=float32)\n",
      "tf.Tensor(0.38734367, shape=(), dtype=float32)\n",
      "tf.Tensor(0.37907133, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3714447, shape=(), dtype=float32)\n",
      "tf.Tensor(0.36438552, shape=(), dtype=float32)\n",
      "tf.Tensor(0.35783786, shape=(), dtype=float32)\n",
      "tf.Tensor(0.35175565, shape=(), dtype=float32)\n",
      "tf.Tensor(0.34609908, shape=(), dtype=float32)\n",
      "tf.Tensor(0.34083253, shape=(), dtype=float32)\n",
      "tf.Tensor(0.33592424, shape=(), dtype=float32)\n",
      "tf.Tensor(0.33134493, shape=(), dtype=float32)\n",
      "tf.Tensor(0.327068, shape=(), dtype=float32)\n",
      "tf.Tensor(0.32306924, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3193268, shape=(), dtype=float32)\n",
      "tf.Tensor(0.3158205, shape=(), dtype=float32)\n",
      "tf.Tensor(0.31253168, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30944377, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30654186, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30381134, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30124006, shape=(), dtype=float32)\n",
      "tf.Tensor(0.29881594, shape=(), dtype=float32)\n",
      "tf.Tensor(0.29652837, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2943676, shape=(), dtype=float32)\n",
      "tf.Tensor(0.29232445, shape=(), dtype=float32)\n",
      "tf.Tensor(0.29039127, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28856033, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28682432, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28517723, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2836128, shape=(), dtype=float32)\n",
      "tf.Tensor(0.28212613, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2807117, shape=(), dtype=float32)\n",
      "tf.Tensor(0.2793653, shape=(), dtype=float32)\n",
      "tf.Tensor(0.27808246, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am Tahsin Mayeesha <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"Tahsin\",\"Mayeesha\"\n",
    "  ]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=100, state_size=128)\n",
    "\n",
    "for _ in range(100):\n",
    "    _ = module.train(tf.constant(sentences))\n",
    "    print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> Hello', b'hello', b'there', b'I', b'am', b'am', b'am', b'am', b'am', b'am', b'am']\n"
     ]
    }
   ],
   "source": [
    " # We have to call this function explicitly if we want it exported, because it\n",
    "  # has no input_signature in the @tf.function decorator.\n",
    "decoded = module.decode_greedy(sequence_length=10, first_word=tf.constant(\"<S> Hello\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 16:23:56.693716  4508 saved_model.py:758] Skipping full serialization of Keras model <__main__.LanguageModel object at 0x000001F0185C14A8>, because its inputs are not defined.\n",
      "W0816 16:23:56.695698  4508 saved_model.py:765] Skipping full serialization of Keras layer <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x000001F0182CD438>, because it is not built.\n",
      "W0816 16:23:56.697700  4508 saved_model.py:765] Skipping full serialization of Keras layer <tensorflow.python.keras.layers.recurrent.RNN object at 0x000001F0197AEC50>, because it is not built.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(module,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.saved_model.load(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> Hello', b'hello', b'there', b'I', b'am', b'am', b'am', b'am', b'am', b'am', b'am']\n"
     ]
    }
   ],
   "source": [
    "d = b.decode_greedy(sequence_length=10,first_word=tf.constant(\"<S> Hello\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

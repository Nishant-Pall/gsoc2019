{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM,Input,Embedding\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.int64)])\n",
    "class LanguageModelEncoder(Model):\n",
    "    def __init__(self,vocab_size,emb_dim,state_size,n_layers):\n",
    "        super(LanguageModelEncoder, self).__init__()\n",
    "        self._state_size = state_size\n",
    "        self.embedding_layer = Embedding(vocab_size,emb_dim)\n",
    "        self._lstm_layers = [LSTM(self._state_size,return_sequences=True) for i in range(n_layers)]\n",
    "        #self._lstm_layer = tf.keras.layers.LSTM(state_size,return_sequences=True)\n",
    "        \n",
    "    def __call__(self,sentence_lookup_ids):\n",
    "        \n",
    "        emb_output = self.embedding_layer(sentence_lookup_ids)\n",
    "        lstm_output = emb_output # initialize to the input\n",
    "        for lstm_layer in self._lstm_layers:\n",
    "            lstm_output = lstm_layer(lstm_output)\n",
    "        return lstm_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocabulary_file(vocabulary):\n",
    "  \"\"\"Write temporary vocab file for module construction.\"\"\"\n",
    "  tmpdir = tempfile.mkdtemp()\n",
    "  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n",
    "  with tf.io.gfile.GFile(vocabulary_file, \"w\") as f:\n",
    "    for entry in vocabulary:\n",
    "      f.write(entry + \"\\n\")\n",
    "  return vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiTModule(tf.train.Checkpoint):\n",
    "  \"\"\"\n",
    "  LATER \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab, emb_dim, buckets, state_size,n_layers):\n",
    "    super(ULMFiTModule, self).__init__()\n",
    "    self._buckets = buckets\n",
    "    self._vocab_size = len(vocab)\n",
    "    self.emb_row_size = self._vocab_size+self._buckets\n",
    "    #self._embeddings = tf.Variable(tf.random.uniform(shape=[self.emb_row_size, emb_dim]))\n",
    "    self._state_size = state_size\n",
    "    self.model = LanguageModelEncoder(self.emb_row_size,emb_dim,state_size,n_layers)\n",
    "    self._vocabulary_file = tracking.TrackableAsset(write_vocabulary_file(vocab)) \n",
    "    self.w2i_table = lookup_ops.index_table_from_file(\n",
    "                    vocabulary_file= self._vocabulary_file,\n",
    "                    num_oov_buckets=self._buckets,\n",
    "                    hasher_spec=lookup_ops.FastHashSpec)\n",
    "    self.i2w_table = lookup_ops.index_to_string_table_from_file(\n",
    "                    vocabulary_file=self._vocabulary_file, \n",
    "                    delimiter = '\\n',\n",
    "                    default_value=\"UNKNOWN\")\n",
    "    self._logit_layer = tf.keras.layers.Dense(self.emb_row_size)\n",
    "\n",
    "\n",
    "    \n",
    "  def _tokenize(self, sentences):\n",
    "    # Perform a minimalistic text preprocessing by removing punctuation and\n",
    "    # splitting on spaces.\n",
    "    normalized_sentences = tf.strings.regex_replace(\n",
    "        input=sentences, pattern=r\"\\pP\", rewrite=\"\")\n",
    "    sparse_tokens = tf.strings.split(normalized_sentences, \" \").to_sparse()\n",
    "\n",
    "    # Deal with a corner case: there is one empty sentence.\n",
    "    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(\"\"))\n",
    "    # Deal with a corner case: all sentences are empty.\n",
    "    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n",
    "\n",
    "    return (sparse_tokens.indices, sparse_tokens.values,\n",
    "            sparse_tokens.dense_shape)\n",
    "    \n",
    "  def _indices_to_words(self, indices):\n",
    "    #return tf.gather(self._vocab_tensor, indices)\n",
    "    return self.i2w_table.lookup(indices)\n",
    "    \n",
    "\n",
    "  def _words_to_indices(self, words):\n",
    "    #return tf.strings.to_hash_bucket(words, self._buckets)\n",
    "    return self.w2i_table.lookup(words)\n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None],tf.dtypes.string)])   \n",
    "  def _tokens_to_lookup_ids(self,sentences):\n",
    "    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n",
    "    tokens_sparse = tf.sparse.SparseTensor(\n",
    "        indices=token_ids, values=token_values, dense_shape=token_dense_shape)\n",
    "    tokens = tf.sparse.to_dense(tokens_sparse, default_value=\"\")\n",
    "\n",
    "    sparse_lookup_ids = tf.sparse.SparseTensor(\n",
    "        indices=tokens_sparse.indices,\n",
    "        values=self._words_to_indices(tokens_sparse.values),\n",
    "        dense_shape=tokens_sparse.dense_shape)\n",
    "    lookup_ids = tf.sparse.to_dense(sparse_lookup_ids, default_value=0)\n",
    "    return tokens,lookup_ids\n",
    "        \n",
    "    \n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n",
    "  def train(self, sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "      #sentence_embeddings = tf.nn.embedding_lookup(self._embeddings,tokens_ids_seq)\n",
    "    \n",
    "      lstm_output = self.model(tokens_ids_seq)\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "      targets = tf.reshape(tokens_ids_target, [-1])\n",
    "      weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "      # Final loss is the mean loss for all token losses.\n",
    "      final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_loss\")\n",
    "\n",
    "    watched = t.watched_variables()\n",
    "    gradients = t.gradient(final_loss, watched)\n",
    "\n",
    "    for w, g in zip(watched, gradients):\n",
    "      w.assign_sub(g)\n",
    "\n",
    "    return final_loss\n",
    "  @tf.function\n",
    "  def return_encoder(self):\n",
    "    return self._model\n",
    "    \n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])  \n",
    "  def validate(self,sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    lstm_output = self.model(tokens_ids_seq)\n",
    "    lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "    logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "    targets = tf.reshape(tokens_ids_target, [-1])\n",
    "    weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "    # Final loss is the mean loss for all token losses.\n",
    "    final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_validation_loss\")\n",
    "\n",
    "    return final_loss\n",
    "    \n",
    "  @tf.function\n",
    "  def decode_greedy(self, sequence_length, first_word):\n",
    "    #initial_state = self._lstm_cell.get_initial_state(\n",
    "    #    dtype=tf.float32, batch_size=1)\n",
    "\n",
    "    sequence = [first_word]\n",
    "    current_word = first_word\n",
    "    current_id = tf.expand_dims(self._words_to_indices(current_word), 0)\n",
    "    #current_state = initial_state\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "      #token_embeddings = tf.nn.embedding_lookup(self._embeddings, current_id)\n",
    "      #token_embeddings = tf.expand_dims(token_embeddings,0)\n",
    "      #logits = self.model(tf.expand_dims(token_embeddings,0))\n",
    "      lstm_output = self.model(tf.expand_dims(current_id,0))\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      softmax = tf.nn.softmax(logits)\n",
    "\n",
    "      next_ids = tf.math.argmax(softmax, axis=1)\n",
    "      next_words = self._indices_to_words(next_ids)[0]\n",
    "      \n",
    "      current_id = next_ids\n",
    "      current_word = next_words\n",
    "      sequence.append(current_word)\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 20:40:09.598569  6400 deprecation.py:323] From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\lookup_ops.py:985: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train loss:  2.7726405  Validation loss  2.7103498\n",
      "Epoch  1  Train loss:  2.6993134  Validation loss  2.6590998\n",
      "Epoch  2  Train loss:  2.638142  Validation loss  2.617051\n",
      "Epoch  3  Train loss:  2.586891  Validation loss  2.5829546\n",
      "Epoch  4  Train loss:  2.5443046  Validation loss  2.5558074\n",
      "Epoch  5  Train loss:  2.5094435  Validation loss  2.534412\n",
      "Epoch  6  Train loss:  2.481242  Validation loss  2.5172582\n",
      "Epoch  7  Train loss:  2.4583495  Validation loss  2.5027583\n",
      "Epoch  8  Train loss:  2.439287  Validation loss  2.489617\n",
      "Epoch  9  Train loss:  2.4227521  Validation loss  2.4770224\n",
      "Epoch  10  Train loss:  2.4078088  Validation loss  2.464573\n",
      "Epoch  11  Train loss:  2.3938692  Validation loss  2.4521012\n",
      "Epoch  12  Train loss:  2.3805819  Validation loss  2.4395401\n",
      "Epoch  13  Train loss:  2.36773  Validation loss  2.4268622\n",
      "Epoch  14  Train loss:  2.3551767  Validation loss  2.4140584\n",
      "Epoch  15  Train loss:  2.3428352  Validation loss  2.401134\n",
      "Epoch  16  Train loss:  2.330652  Validation loss  2.3881083\n",
      "Epoch  17  Train loss:  2.318598  Validation loss  2.3750083\n",
      "Epoch  18  Train loss:  2.3066595  Validation loss  2.3618677\n",
      "Epoch  19  Train loss:  2.2948318  Validation loss  2.348721\n",
      "Epoch  20  Train loss:  2.2831154  Validation loss  2.3356047\n",
      "Epoch  21  Train loss:  2.271512  Validation loss  2.3225503\n",
      "Epoch  22  Train loss:  2.260025  Validation loss  2.309586\n",
      "Epoch  23  Train loss:  2.2486563  Validation loss  2.2967381\n",
      "Epoch  24  Train loss:  2.237406  Validation loss  2.284024\n",
      "Epoch  25  Train loss:  2.2262719  Validation loss  2.2714577\n",
      "Epoch  26  Train loss:  2.2152498  Validation loss  2.259047\n",
      "Epoch  27  Train loss:  2.2043338  Validation loss  2.2467947\n",
      "Epoch  28  Train loss:  2.1935143  Validation loss  2.234698\n",
      "Epoch  29  Train loss:  2.1827812  Validation loss  2.2227504\n",
      "Epoch  30  Train loss:  2.1721213  Validation loss  2.2109404\n",
      "Epoch  31  Train loss:  2.1615205  Validation loss  2.1992533\n",
      "Epoch  32  Train loss:  2.1509635  Validation loss  2.1876717\n",
      "Epoch  33  Train loss:  2.1404321  Validation loss  2.1761749\n",
      "Epoch  34  Train loss:  2.1299083  Validation loss  2.1647403\n",
      "Epoch  35  Train loss:  2.1193714  Validation loss  2.153342\n",
      "Epoch  36  Train loss:  2.1088  Validation loss  2.141954\n",
      "Epoch  37  Train loss:  2.0981705  Validation loss  2.1305463\n",
      "Epoch  38  Train loss:  2.0874584  Validation loss  2.1190884\n",
      "Epoch  39  Train loss:  2.076636  Validation loss  2.107546\n",
      "Epoch  40  Train loss:  2.065674  Validation loss  2.0958834\n",
      "Epoch  41  Train loss:  2.0545404  Validation loss  2.0840616\n",
      "Epoch  42  Train loss:  2.0432  Validation loss  2.0720384\n",
      "Epoch  43  Train loss:  2.0316153  Validation loss  2.0597684\n",
      "Epoch  44  Train loss:  2.019744  Validation loss  2.0472035\n",
      "Epoch  45  Train loss:  2.007542  Validation loss  2.0342896\n",
      "Epoch  46  Train loss:  1.9949596  Validation loss  2.0209703\n",
      "Epoch  47  Train loss:  1.9819428  Validation loss  2.0071847\n",
      "Epoch  48  Train loss:  1.9684354  Validation loss  1.9928691\n",
      "Epoch  49  Train loss:  1.9543762  Validation loss  1.9779557\n",
      "Epoch  50  Train loss:  1.939701  Validation loss  1.9623765\n",
      "Epoch  51  Train loss:  1.924344  Validation loss  1.9460633\n",
      "Epoch  52  Train loss:  1.9082397  Validation loss  1.928952\n",
      "Epoch  53  Train loss:  1.8913251  Validation loss  1.9109851\n",
      "Epoch  54  Train loss:  1.873544  Validation loss  1.8921174\n",
      "Epoch  55  Train loss:  1.8548516  Validation loss  1.8723215\n",
      "Epoch  56  Train loss:  1.8352207  Validation loss  1.8515923\n",
      "Epoch  57  Train loss:  1.8146461  Validation loss  1.8299502\n",
      "Epoch  58  Train loss:  1.7931529  Validation loss  1.8074455\n",
      "Epoch  59  Train loss:  1.770796  Validation loss  1.7841543\n",
      "Epoch  60  Train loss:  1.7476636  Validation loss  1.7601751\n",
      "Epoch  61  Train loss:  1.7238704  Validation loss  1.7356182\n",
      "Epoch  62  Train loss:  1.6995484  Validation loss  1.7105978\n",
      "Epoch  63  Train loss:  1.6748352  Validation loss  1.6852221\n",
      "Epoch  64  Train loss:  1.6498609  Validation loss  1.6595892\n",
      "Epoch  65  Train loss:  1.6247375  Validation loss  1.6337823\n",
      "Epoch  66  Train loss:  1.5995542  Validation loss  1.607875\n",
      "Epoch  67  Train loss:  1.5743783  Validation loss  1.5819265\n",
      "Epoch  68  Train loss:  1.5492561  Validation loss  1.5559916\n",
      "Epoch  69  Train loss:  1.5242203  Validation loss  1.5301081\n",
      "Epoch  70  Train loss:  1.4992925  Validation loss  1.5043182\n",
      "Epoch  71  Train loss:  1.4744866  Validation loss  1.4786339\n",
      "Epoch  72  Train loss:  1.4498109  Validation loss  1.4530941\n",
      "Epoch  73  Train loss:  1.4252697  Validation loss  1.4276711\n",
      "Epoch  74  Train loss:  1.4008651  Validation loss  1.402432\n",
      "Epoch  75  Train loss:  1.3765961  Validation loss  1.3772608\n",
      "Epoch  76  Train loss:  1.3524625  Validation loss  1.3523642\n",
      "Epoch  77  Train loss:  1.328463  Validation loss  1.3273441\n",
      "Epoch  78  Train loss:  1.3045995  Validation loss  1.3029422\n",
      "Epoch  79  Train loss:  1.280876  Validation loss  1.2777373\n",
      "Epoch  80  Train loss:  1.2573072  Validation loss  1.2545384\n",
      "Epoch  81  Train loss:  1.2339264  Validation loss  1.2280241\n",
      "Epoch  82  Train loss:  1.2108696  Validation loss  1.209688\n",
      "Epoch  83  Train loss:  1.1886277  Validation loss  1.1799285\n",
      "Epoch  84  Train loss:  1.1692104  Validation loss  1.1946995\n",
      "Epoch  85  Train loss:  1.1618088  Validation loss  1.1999984\n",
      "Epoch  86  Train loss:  1.1972713  Validation loss  1.5967591\n",
      "Epoch  87  Train loss:  1.4516428  Validation loss  1.9100022\n",
      "Epoch  88  Train loss:  1.8257571  Validation loss  1.9671639\n",
      "Epoch  89  Train loss:  1.8048356  Validation loss  1.496375\n",
      "Epoch  90  Train loss:  1.4879948  Validation loss  1.2086163\n",
      "Epoch  91  Train loss:  1.1336489  Validation loss  1.0627358\n",
      "Epoch  92  Train loss:  1.0636806  Validation loss  1.105166\n",
      "Epoch  93  Train loss:  1.0505803  Validation loss  1.0118746\n",
      "Epoch  94  Train loss:  1.0161635  Validation loss  1.0600729\n",
      "Epoch  95  Train loss:  1.0090387  Validation loss  0.9680199\n",
      "Epoch  96  Train loss:  0.97673017  Validation loss  1.0189031\n",
      "Epoch  97  Train loss:  0.9716562  Validation loss  0.9292559\n",
      "Epoch  98  Train loss:  0.9424421  Validation loss  0.98616064\n",
      "Epoch  99  Train loss:  0.94035345  Validation loss  0.8928703\n",
      "Epoch  100  Train loss:  0.9108271  Validation loss  0.9551206\n",
      "Epoch  101  Train loss:  0.9109145  Validation loss  0.8582557\n",
      "Epoch  102  Train loss:  0.8808668  Validation loss  0.92424214\n",
      "Epoch  103  Train loss:  0.8822553  Validation loss  0.8248645\n",
      "Epoch  104  Train loss:  0.85185736  Validation loss  0.8920508\n",
      "Epoch  105  Train loss:  0.85336965  Validation loss  0.79261184\n",
      "Epoch  106  Train loss:  0.8235136  Validation loss  0.8583543\n",
      "Epoch  107  Train loss:  0.8240696  Validation loss  0.7615173\n",
      "Epoch  108  Train loss:  0.79571474  Validation loss  0.8233484\n",
      "Epoch  109  Train loss:  0.7944289  Validation loss  0.7315989\n",
      "Epoch  110  Train loss:  0.7683533  Validation loss  0.7873354\n",
      "Epoch  111  Train loss:  0.76459616  Validation loss  0.7027731\n",
      "Epoch  112  Train loss:  0.7412359  Validation loss  0.7505108\n",
      "Epoch  113  Train loss:  0.7346607  Validation loss  0.67488164\n",
      "Epoch  114  Train loss:  0.71409535  Validation loss  0.7130593\n",
      "Epoch  115  Train loss:  0.7047259  Validation loss  0.64785516\n",
      "Epoch  116  Train loss:  0.68674845  Validation loss  0.6755085\n",
      "Epoch  117  Train loss:  0.6751583  Validation loss  0.6220573\n",
      "Epoch  118  Train loss:  0.6594875  Validation loss  0.63930917\n",
      "Epoch  119  Train loss:  0.646987  Validation loss  0.5986103\n",
      "Epoch  120  Train loss:  0.63360316  Validation loss  0.607011\n",
      "Epoch  121  Train loss:  0.6219771  Validation loss  0.57883143\n",
      "Epoch  122  Train loss:  0.61108106  Validation loss  0.5807533\n",
      "Epoch  123  Train loss:  0.601448  Validation loss  0.562611\n",
      "Epoch  124  Train loss:  0.59275436  Validation loss  0.5602686\n",
      "Epoch  125  Train loss:  0.584868  Validation loss  0.5484502\n",
      "Epoch  126  Train loss:  0.5775782  Validation loss  0.54371667\n",
      "Epoch  127  Train loss:  0.5707399  Validation loss  0.5352415\n",
      "Epoch  128  Train loss:  0.56425625  Validation loss  0.52955645\n",
      "Epoch  129  Train loss:  0.55806845  Validation loss  0.5227357\n",
      "Epoch  130  Train loss:  0.5521435  Validation loss  0.51695037\n",
      "Epoch  131  Train loss:  0.54646015  Validation loss  0.5110015\n",
      "Epoch  132  Train loss:  0.54100436  Validation loss  0.5054844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  133  Train loss:  0.53576446  Validation loss  0.5000903\n",
      "Epoch  134  Train loss:  0.53073055  Validation loss  0.49494675\n",
      "Epoch  135  Train loss:  0.52589273  Validation loss  0.48998278\n",
      "Epoch  136  Train loss:  0.5212422  Validation loss  0.48521474\n",
      "Epoch  137  Train loss:  0.5167701  Validation loss  0.48062405\n",
      "Epoch  138  Train loss:  0.5124686  Validation loss  0.47620535\n",
      "Epoch  139  Train loss:  0.50832945  Validation loss  0.47194996\n",
      "Epoch  140  Train loss:  0.50434583  Validation loss  0.4678505\n",
      "Epoch  141  Train loss:  0.5005102  Validation loss  0.4638999\n",
      "Epoch  142  Train loss:  0.49681613  Validation loss  0.46009117\n",
      "Epoch  143  Train loss:  0.49325705  Validation loss  0.45641845\n",
      "Epoch  144  Train loss:  0.48982728  Validation loss  0.4528754\n",
      "Epoch  145  Train loss:  0.48652104  Validation loss  0.44945642\n",
      "Epoch  146  Train loss:  0.48333263  Validation loss  0.44615623\n",
      "Epoch  147  Train loss:  0.4802572  Validation loss  0.44296938\n",
      "Epoch  148  Train loss:  0.4772898  Validation loss  0.43989152\n",
      "Epoch  149  Train loss:  0.47442588  Validation loss  0.4369179\n",
      "Epoch  150  Train loss:  0.471661  Validation loss  0.43404406\n",
      "Epoch  151  Train loss:  0.4689909  Validation loss  0.43126625\n",
      "Epoch  152  Train loss:  0.4664119  Validation loss  0.42858028\n",
      "Epoch  153  Train loss:  0.46391994  Validation loss  0.42598268\n",
      "Epoch  154  Train loss:  0.46151173  Validation loss  0.42346993\n",
      "Epoch  155  Train loss:  0.4591838  Validation loss  0.42103866\n",
      "Epoch  156  Train loss:  0.45693284  Validation loss  0.41868538\n",
      "Epoch  157  Train loss:  0.45475578  Validation loss  0.41640756\n",
      "Epoch  158  Train loss:  0.45264977  Validation loss  0.41420254\n",
      "Epoch  159  Train loss:  0.4506122  Validation loss  0.4120666\n",
      "Epoch  160  Train loss:  0.44864005  Validation loss  0.40999815\n",
      "Epoch  161  Train loss:  0.4467311  Validation loss  0.40799418\n",
      "Epoch  162  Train loss:  0.44488266  Validation loss  0.40605223\n",
      "Epoch  163  Train loss:  0.44309273  Validation loss  0.40417042\n",
      "Epoch  164  Train loss:  0.44135877  Validation loss  0.4023461\n",
      "Epoch  165  Train loss:  0.43967897  Validation loss  0.4005776\n",
      "Epoch  166  Train loss:  0.43805125  Validation loss  0.3988628\n",
      "Epoch  167  Train loss:  0.43647355  Validation loss  0.3971995\n",
      "Epoch  168  Train loss:  0.43494412  Validation loss  0.3955864\n",
      "Epoch  169  Train loss:  0.43346116  Validation loss  0.39402106\n",
      "Epoch  170  Train loss:  0.43202302  Validation loss  0.3925023\n",
      "Epoch  171  Train loss:  0.4306281  Validation loss  0.39102837\n",
      "Epoch  172  Train loss:  0.42927468  Validation loss  0.38959774\n",
      "Epoch  173  Train loss:  0.42796147  Validation loss  0.3882088\n",
      "Epoch  174  Train loss:  0.42668694  Validation loss  0.38685992\n",
      "Epoch  175  Train loss:  0.42544976  Validation loss  0.38555\n",
      "Epoch  176  Train loss:  0.4242485  Validation loss  0.3842778\n",
      "Epoch  177  Train loss:  0.42308205  Validation loss  0.383042\n",
      "Epoch  178  Train loss:  0.4219492  Validation loss  0.3818411\n",
      "Epoch  179  Train loss:  0.42084864  Validation loss  0.38067394\n",
      "Epoch  180  Train loss:  0.4197793  Validation loss  0.37953964\n",
      "Epoch  181  Train loss:  0.41874018  Validation loss  0.378437\n",
      "Epoch  182  Train loss:  0.4177302  Validation loss  0.37736472\n",
      "Epoch  183  Train loss:  0.41674832  Validation loss  0.3763222\n",
      "Epoch  184  Train loss:  0.4157936  Validation loss  0.3753082\n",
      "Epoch  185  Train loss:  0.41486523  Validation loss  0.37432167\n",
      "Epoch  186  Train loss:  0.4139622  Validation loss  0.373362\n",
      "Epoch  187  Train loss:  0.41308367  Validation loss  0.3724281\n",
      "Epoch  188  Train loss:  0.41222885  Validation loss  0.37151924\n",
      "Epoch  189  Train loss:  0.41139698  Validation loss  0.37063438\n",
      "Epoch  190  Train loss:  0.41058728  Validation loss  0.36977318\n",
      "Epoch  191  Train loss:  0.40979898  Validation loss  0.3689343\n",
      "Epoch  192  Train loss:  0.4090314  Validation loss  0.36811742\n",
      "Epoch  193  Train loss:  0.4082839  Validation loss  0.3673217\n",
      "Epoch  194  Train loss:  0.40755576  Validation loss  0.36654657\n",
      "Epoch  195  Train loss:  0.4068465  Validation loss  0.36579114\n",
      "Epoch  196  Train loss:  0.40615523  Validation loss  0.3650553\n",
      "Epoch  197  Train loss:  0.4054818  Validation loss  0.36433786\n",
      "Epoch  198  Train loss:  0.40482524  Validation loss  0.36363813\n",
      "Epoch  199  Train loss:  0.40418518  Validation loss  0.3629562\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\",\n",
    "             \"<S> hello world <E>\", \"<S> who are you? <E>\"]\n",
    "validation_sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"fine\",\"thank\",\"world\",\n",
    "    \"who\"\n",
    "  ]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=1, state_size=128,n_layers=1)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = module.train(tf.constant(sentences))\n",
    "    validation_loss = module.validate(tf.constant(validation_sentences))\n",
    "    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy(),\" Validation loss \",validation_loss.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> you', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>']\n"
     ]
    }
   ],
   "source": [
    " # We have to call this function explicitly if we want it exported, because it\n",
    "  # has no input_signature in the @tf.function decorator.\n",
    "decoded = module.decode_greedy(sequence_length=10, first_word=tf.constant(\"<S> you\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 20:41:57.326647  6400 saved_model.py:758] Skipping full serialization of Keras model <__main__.LanguageModelEncoder object at 0x000001A6A8210550>, because its inputs are not defined.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(module,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.saved_model.load(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> you', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>', b'world', b'<E>']\n"
     ]
    }
   ],
   "source": [
    "d = b.decode_greedy(sequence_length=10,first_word=tf.constant(\"<S> Hello\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Head \n",
    "\n",
    "\n",
    "Classifier head takes in the final layer output of the languaage model and first gets the average pool and max pool of the \n",
    "final layer outputs, then passes the concatanation of last time steps hidden state, max pool results and average pool results through given number Dense-dropout-batchnormalization blocks. Finally it produces the classifier output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(Model):\n",
    "    def __init__(self,language_module,num_labels,dense_units=(128,128),dropouts=(0.1,0.1)):\n",
    "        \n",
    "        # initialization stuff\n",
    "        super(LanguageClassifier,self).__init__()\n",
    "        self._language_module = language_module\n",
    "        self.model_encoder = language_module.model\n",
    "        \n",
    "        \n",
    "        # classifier head layers\n",
    "        self.dense_layers = [Dense(units,activation=\"relu\") for units in dense_units]\n",
    "        self.dropout_layers = [Dropout(p) for p in dropouts]\n",
    "        self.max_pool_layer = GlobalMaxPooling1D()\n",
    "        self.average_pool_layer = GlobalAveragePooling1D()\n",
    "        self.batchnorm_layer = BatchNormalization()\n",
    "        self.n_layers = len(self.dense_layers)\n",
    "        self.final_layer = Dense(num_labels,activation=\"sigmoid\")\n",
    "        \n",
    "    def __call__(self,sentences):\n",
    "        \n",
    "        tokens,lookup_ids = self._language_module._tokens_to_lookup_ids(sentences)\n",
    "        print(lookup_ids.dtype)\n",
    "        self.enc_out = self.model_encoder(lookup_ids)\n",
    "        last_h = self.enc_out[:,-1,:]\n",
    "        max_pool_output = self.max_pool_layer(self.enc_out)\n",
    "        average_pool_output = self.average_pool_layer(self.enc_out)\n",
    "        \n",
    "        output = concatenate([last_h,max_pool_output,average_pool_output])\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = self.dense_layers[i](output)\n",
    "            output = self.dropout_layers[i](output)\n",
    "            output = self.batchnorm_layer(output)\n",
    "        \n",
    "        final_output = self.final_layer(output)\n",
    "        return final_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageClassifier(num_labels=2,language_module=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.LanguageModelEncoder at 0x1a6a8210550>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1a6c6b3ae48>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1a6c6b3aa20>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1a6bcd9f908>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1a6bd153518>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D at 0x1a6c6b66048>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D at 0x1a6c6b661d0>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1a6c6b663c8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1a6c6b666d8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "probabilities = model(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=30124, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.51512676, 0.5485683 ],\n",
       "       [0.52851266, 0.55098623],\n",
       "       [0.56506807, 0.50878805],\n",
       "       [0.51537895, 0.54841405],\n",
       "       [0.5163129 , 0.53004086]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 20:43:08.432374  6400 util.py:244] Unresolved object in checkpoint: (root).i2w_table._initializer\n",
      "W0818 20:43:08.432374  6400 util.py:244] Unresolved object in checkpoint: (root).i2w_table._initializer._filename\n",
      "W0818 20:43:08.432374  6400 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n",
      "Epoch 1, Loss: 0.7108809351921082, Accuracy: 20.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 2, Loss: 0.6584046483039856, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 3, Loss: 0.6259582042694092, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 4, Loss: 0.5955281853675842, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 5, Loss: 0.5612379908561707, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "labels = tf.constant([[1],[0],[1],[0],[0]])\n",
    "@tf.function\n",
    "def train_step(samples, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(samples)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_step(sentences, labels)\n",
    "    #test_step(validation_sentences, validation_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "  # Reset the metrics for the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (probabilities.numpy() > 0.5).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

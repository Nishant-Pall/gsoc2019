{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Configurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Language model Training on 28000+ wiki articles, Validating on ___ wiki articles\n",
    "* vocabulary size : 10000, min_freq = 3\n",
    "* Language Model finetuning on ____ imdb articles, \n",
    "* Classifier training on _____ imdb reviews , validation on ____ imdb reviews\n",
    "* embedding size = \n",
    "* number of lstm units\n",
    "* lstm layer number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM,Input,Embedding\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imdb_reviews.pkl', 'rb') as f:\n",
    "    imdb_reviews = pickle.load(f)\n",
    "    \n",
    "with open('data/imdb_labels.pkl', 'rb') as f:\n",
    "    imdb_labels = pickle.load(f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imdb_valid_reviews.pkl', 'rb') as f:\n",
    "    imdb_valid_reviews = pickle.load(f)\n",
    "    \n",
    "with open('data/imdb_valid_labels.pkl', 'rb') as f:\n",
    "    imdb_valid_labels = pickle.load(f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_labels = tf.keras.utils.to_categorical(imdb_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_valid_labels = tf.keras.utils.to_categorical(imdb_valid_labels,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Tensorflow Datasets for Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = tf.data.Dataset.from_tensor_slices(imdb_reviews).batch(16,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset_labels = tf.data.Dataset.from_tensor_slices(imdb_labels).batch(16,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_set = tf.data.Dataset.zip((imdb_dataset,imdb_dataset_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 10:50:39.162348 140467105605376 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/lookup_ops.py:985: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train loss:  2.771884  Validation loss  2.7692854\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\",\n",
    "             \"<S> hello world <E>\", \"<S> who are you? <E>\"]\n",
    "validation_sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"fine\",\"thank\",\"world\",\"who\"]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=1, state_size=128,n_layers=1)\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_loss = module.train(tf.constant(sentences))\n",
    "    validation_loss = module.validate(tf.constant(validation_sentences))\n",
    "    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy(),\" Validation loss \",validation_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = tf.saved_model.load(\"finetuned_language_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Head \n",
    "\n",
    "\n",
    "Classifier head takes in the final layer output of the languaage model and first gets the average pool and max pool of the \n",
    "final layer outputs, then passes the concatanation of last time steps hidden state, max pool results and average pool results through given number Dense-dropout-batchnormalization blocks. Finally it produces the classifier output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(Model):\n",
    "    def __init__(self,language_module,num_labels,dense_units=(128,128),dropouts=(0.1,0.1)):\n",
    "        \n",
    "        # initialization stuff\n",
    "        super(LanguageClassifier,self).__init__()\n",
    "        self._language_module = language_module\n",
    "        self.model_encoder = language_module.model\n",
    "        \n",
    "        \n",
    "        # classifier head layers\n",
    "        self.dense_layers = [Dense(units,activation=\"relu\") for units in dense_units]\n",
    "        self.dropout_layers = [Dropout(p) for p in dropouts]\n",
    "        self.max_pool_layer = GlobalMaxPooling1D()\n",
    "        self.average_pool_layer = GlobalAveragePooling1D()\n",
    "        self.batchnorm_layer = BatchNormalization()\n",
    "        self.n_layers = len(self.dense_layers)\n",
    "        self.final_layer = Dense(num_labels,activation=\"sigmoid\")\n",
    "        \n",
    "    def __call__(self,sentences):\n",
    "        \n",
    "        tokens,lookup_ids = self._language_module._tokens_to_lookup_ids(sentences)\n",
    "        self.enc_out = self.model_encoder(lookup_ids)\n",
    "        last_h = self.enc_out[:,-1,:]\n",
    "        max_pool_output = self.max_pool_layer(self.enc_out)\n",
    "        average_pool_output = self.average_pool_layer(self.enc_out)\n",
    "        \n",
    "        output = concatenate([last_h,max_pool_output,average_pool_output])\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = self.dense_layers[i](output)\n",
    "            #output = self.dropout_layers[i](output)\n",
    "            output = self.batchnorm_layer(output)\n",
    "        \n",
    "        final_output = self.final_layer(output)\n",
    "        return final_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageClassifier(language_module=module,num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = []\n",
    "train_accuracy_hist = []\n",
    "\n",
    "def track(tl_score,tl_accuracy):\n",
    "    train_loss_hist.append(tl_score)\n",
    "    train_accuracy_hist.append(tl_accuracy)\n",
    "\n",
    "@tf.function\n",
    "def train_step(samples, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(samples)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  watched = tape.watched_variables()\n",
    "  gradients = tape.gradient(loss, watched)\n",
    "  optimizer.apply_gradients(zip(gradients, watched))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "  return loss, train_accuracy(labels,predictions)\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(samples, labels):\n",
    "  predictions = model(samples)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0  loss  tf.Tensor(0.677494, shape=(), dtype=float32)  Accuracy  56.25\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for reviews,labels in imdb_train_set:\n",
    "        loss,acc = train_step(reviews, labels)\n",
    "        track(loss,acc)\n",
    "        if step%500==0:\n",
    "            print(\"Step \",step, \" loss \",loss,\" Accuracy \", acc.numpy()*100)\n",
    "        step+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier was not trained, but I couldn't get the gains I was supposed to get by starting from finetuned language model. Language model loss was around 4.9 after finetuning on Imdb, but I'm probably updating the gradients at a higher learning rate than needed. Original ULMFiT uses a highly regularized AWD-LSTM model while this one is a vanilla LSTM network. And also during finetuning I should probably be reducing the learning rate and do gradual unfreezing of layers , in the current implementation the classifier is losing the knowledge gained during language model training because of larger learning rate. More experiments with smaller learning rates will also be beneficial. Comparing how long this model takes to converge after being started from random weights vs finetuned weights will also be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_losses.pkl', 'rb') as f:\n",
    "    train_losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [l.numpy() for l in train_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.Series(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_los = []\n",
    "for i,v in enumerate(losses):\n",
    "    if i%500==0:\n",
    "        train_los.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.Series(train_los).plot()\n",
    "plt.xlabel(\"Number of Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "ax.set_xticklabels(labels=['0','500','1000','1500','2000','2500','3000','3500']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "Jeremy Howard and Sebastian Ruder introduced ULMFiT in 2018. Paper link : https://arxiv.org/abs/1801.06146\n",
    "\n",
    "Key steps of ULMFiT Idea : \n",
    "\n",
    "* Training a language model on a large corpus\n",
    "* Fine-tuning the language model on the data that will be used in the downstream task like text classification\n",
    "* Adding a classifier head on top of the language model and fine-tune again for classification\n",
    "\n",
    "There are more details : \n",
    "\n",
    "* The model Fastai used initially is AWD-LSTM from Steven Merity's [paper](https://arxiv.org/abs/1708.02182)\n",
    "* There are many regularization methods used in their implementation like using weight dropout, embedding dropout\n",
    ",activation regularization,dropout on input and output layers and so on.\n",
    "* Fine-tuning is done by varying learning rates. \n",
    "\n",
    "In this case first we try to stick to the basics. This implementation has a language module which will be \n",
    "trained on wikitext-103 dataset initially, fine-tuned on imdb dataset and ultimately used for sentiment classification with imdb dataset. For demo I'm only using fake data to show the process, the model has not been trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ulmfit_approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM,Input,Embedding\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data for Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-19 09:45:58--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.99.117\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.99.117|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 190229076 (181M) [application/zip]\n",
      "Saving to: ‘wikitext-103-v1.zip’\n",
      "\n",
      "wikitext-103-v1.zip 100%[===================>] 181.42M  1.77MB/s    in 3m 8s   \n",
      "\n",
      "2019-08-19 09:49:07 (986 KB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Wikitext 103\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-103-v1.zip\n",
      "   creating: wikitext-103/\n",
      "  inflating: wikitext-103/wiki.test.tokens  \n",
      "  inflating: wikitext-103/wiki.valid.tokens  \n",
      "  inflating: wikitext-103/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!unzip wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IMDB Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-19 09:49:21--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  48.5MB/s    in 1.7s    \n",
      "\n",
      "2019-08-19 09:49:23 (48.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb\t\t   Full ULMFiT.ipynb\t\t   wikitext-103\n",
      "aclImdb_v1.tar.gz  ulmfit_module_classifier.ipynb  wikitext-103-v1.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Wikitext-103\n",
    "\n",
    "\n",
    "The data is in a long .txt file, so we need to split that into articles before language modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"wikitext-103/wiki.train.tokens\"\n",
    "valid_path = \"wikitext-103/wiki.valid.tokens\"\n",
    "test_path = \"wikitext-103/wiki.test.tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ = [^=]* = $', line)) != 0\n",
    "    \n",
    "UNK = \"unk\"\n",
    "def read_wiki(filename):\n",
    "    articles = []\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = ''\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article += line\n",
    "        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n",
    "            current_article = current_article.replace('<unk>', UNK)\n",
    "            articles.append(current_article)\n",
    "            current_article = ''\n",
    "    current_article = current_article.replace('<unk>', UNK)\n",
    "    articles.append(current_article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  x = x.strip().lower()\n",
    "  \n",
    "  # fix html \n",
    "  re1 = re.compile(r'  +')\n",
    "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
    "  x=re1.sub(' ', html.unescape(x))\n",
    "  \n",
    "  \"Add spaces around / and # in `t`. \\n\" \n",
    "  x=re.sub(r'([/#\\n])', r' \\1 ', x)\n",
    "  \n",
    "  \"Remove multiple spaces in `t`.\"\n",
    "  \n",
    "  x=re.sub(' {2,}', ' ', x)\n",
    "  \n",
    "  return '<S> '+x+' <E>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = read_wiki(train_path)\n",
    "wiki_valid_articles = read_wiki(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = [preprocess(article) for article in wiki_articles]\n",
    "wiki_valid_articles = [preprocess(article) for article in wiki_valid_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> = gambia women 's national football team = \n",
      " \n",
      " the gambia women 's national football team represents the gambia in international football competition . the team , however , has not competed in a match recognised by fifa , the sport 's international governing body , despite that organised women '\n"
     ]
    }
   ],
   "source": [
    "print(wiki_articles[3][0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(data,max_words,min_freq):\n",
    "    counter = Counter([word for para in data for word in para.split()])\n",
    "    vocab = [word[0] for word in counter.most_common() if word[1] >= min_freq]\n",
    "    return vocab[:max_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = build_vocab(wiki_articles,max_words=10000,min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess IMDB Dataset\n",
    "\n",
    "labels :  0 for positive, 1 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_names = ['pos','neg']\n",
    "\n",
    "imdb_file_paths = []\n",
    "imdb_labels = []\n",
    "for i, dir in enumerate(dir_names):\n",
    "  file_names = [os.path.join(\"aclImdb/train\",dir,name) for name in os.listdir(\"aclImdb/train/\"+dir)]\n",
    "  imdb_file_paths += file_names\n",
    "  imdb_labels += [i]*len(os.listdir(\"aclImdb/train/\"+dir))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "permutation = np.random.permutation(len(imdb_file_paths))\n",
    "\n",
    "imdb_file_paths = np.array(imdb_file_paths)[permutation]\n",
    "imdb_labels = np.array(imdb_labels)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(imdb_file_paths),len(imdb_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_reviews = []\n",
    "for file in imdb_file_paths:\n",
    "  with open(file,encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "  data = preprocess(data)\n",
    "  imdb_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> an end of an era was released here in the states in spring 2002 with \"the rookie,\" a disney live action film that seemed to be the \"best for last!!!!!\" it took place right here in texas! actually, the story began in west texas, as evidenced by an area code found on a sign over there. it was about a high school coach who was so convinced by his high class baseball team that he decided to go professional!!!!! \n",
      " \n",
      " what i liked about this movie: it was sooo nice!!!!! it was a very good sports movie, ala \"the mighty ducks\" trilogy. it had also taken moviegoers across texas, from somewhere between the panhandle and el paso all the way to the metroplex (where i live). i can tell because i recognize that ballpark (was \"the ballpark in arlington;\" now it's \"ameriquest field\")! it was nice to see disney's \"golden age\" end here in my area!!!!! \n",
      " \n",
      " r.i.p. \n",
      " \n",
      " golden age of disney \n",
      " \n",
      " 1920s-spring 2002 \n",
      " \n",
      " \"it all started with a mouse...and it ended with baseball.\" (sobs) \n",
      " \n",
      " 10 / 10 <E> 0\n"
     ]
    }
   ],
   "source": [
    "print(imdb_reviews[0],imdb_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelEncoder(tf.train.Checkpoint):\n",
    "    def __init__(self,vocab_size,emb_dim,state_size,n_layers):\n",
    "        super(LanguageModelEncoder, self).__init__()\n",
    "        self._state_size = state_size\n",
    "        self.embedding_layer = Embedding(vocab_size,emb_dim)\n",
    "        self._lstm_layers = [LSTM(self._state_size,return_sequences=True) for i in range(n_layers)]\n",
    "        \n",
    "    @tf.function(input_signature=[tf.TensorSpec([None,None], tf.dtypes.int64)])\n",
    "    def __call__(self,sentence_lookup_ids):\n",
    "        \n",
    "        emb_output = self.embedding_layer(sentence_lookup_ids)\n",
    "        lstm_output = emb_output # initialize to the input\n",
    "        for lstm_layer in self._lstm_layers:\n",
    "            lstm_output = lstm_layer(lstm_output)\n",
    "        return lstm_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocabulary_file(vocabulary):\n",
    "  \"\"\"Write temporary vocab file for module construction.\"\"\"\n",
    "  tmpdir = tempfile.mkdtemp()\n",
    "  vocabulary_file = os.path.join(tmpdir, \"tokens.txt\")\n",
    "  with tf.io.gfile.GFile(vocabulary_file, \"w\") as f:\n",
    "    for entry in vocabulary:\n",
    "      f.write(entry + \"\\n\")\n",
    "  return vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiTModule(tf.train.Checkpoint):\n",
    "  \"\"\"\n",
    "  Trains a language model on given sentences\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab, emb_dim, buckets, state_size,n_layers):\n",
    "    super(ULMFiTModule, self).__init__()\n",
    "    self._buckets = buckets\n",
    "    self._vocab_size = len(vocab)\n",
    "    self.emb_row_size = self._vocab_size+self._buckets\n",
    "    #self._embeddings = tf.Variable(tf.random.uniform(shape=[self.emb_row_size, emb_dim]))\n",
    "    self._state_size = state_size\n",
    "    self.model = LanguageModelEncoder(self.emb_row_size,emb_dim,state_size,n_layers)\n",
    "    self._vocabulary_file = tracking.TrackableAsset(write_vocabulary_file(vocab)) \n",
    "    self.w2i_table = lookup_ops.index_table_from_file(\n",
    "                    vocabulary_file= self._vocabulary_file,\n",
    "                    num_oov_buckets=self._buckets,\n",
    "                    hasher_spec=lookup_ops.FastHashSpec)\n",
    "    self.i2w_table = lookup_ops.index_to_string_table_from_file(\n",
    "                    vocabulary_file=self._vocabulary_file, \n",
    "                    delimiter = '\\n',\n",
    "                    default_value=\"UNKNOWN\")\n",
    "    self._logit_layer = tf.keras.layers.Dense(self.emb_row_size)\n",
    "    self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "    \n",
    "  def _tokenize(self, sentences):\n",
    "    # Perform a minimalistic text preprocessing by removing punctuation and\n",
    "    # splitting on spaces.\n",
    "    normalized_sentences = tf.strings.regex_replace(\n",
    "        input=sentences, pattern=r\"\\pP\", rewrite=\"\")\n",
    "    sparse_tokens = tf.strings.split(normalized_sentences, \" \").to_sparse()\n",
    "\n",
    "    # Deal with a corner case: there is one empty sentence.\n",
    "    sparse_tokens, _ = tf.sparse.fill_empty_rows(sparse_tokens, tf.constant(\"\"))\n",
    "    # Deal with a corner case: all sentences are empty.\n",
    "    sparse_tokens = tf.sparse.reset_shape(sparse_tokens)\n",
    "\n",
    "    return (sparse_tokens.indices, sparse_tokens.values,\n",
    "            sparse_tokens.dense_shape)\n",
    "    \n",
    "  def _indices_to_words(self, indices):\n",
    "    #return tf.gather(self._vocab_tensor, indices)\n",
    "    return self.i2w_table.lookup(indices)\n",
    "    \n",
    "\n",
    "  def _words_to_indices(self, words):\n",
    "    #return tf.strings.to_hash_bucket(words, self._buckets)\n",
    "    return self.w2i_table.lookup(words)\n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None],tf.dtypes.string)])   \n",
    "  def _tokens_to_lookup_ids(self,sentences):\n",
    "    token_ids, token_values, token_dense_shape = self._tokenize(sentences)\n",
    "    tokens_sparse = tf.sparse.SparseTensor(\n",
    "        indices=token_ids, values=token_values, dense_shape=token_dense_shape)\n",
    "    tokens = tf.sparse.to_dense(tokens_sparse, default_value=\"\")\n",
    "\n",
    "    sparse_lookup_ids = tf.sparse.SparseTensor(\n",
    "        indices=tokens_sparse.indices,\n",
    "        values=self._words_to_indices(tokens_sparse.values),\n",
    "        dense_shape=tokens_sparse.dense_shape)\n",
    "    lookup_ids = tf.sparse.to_dense(sparse_lookup_ids, default_value=0)\n",
    "    return tokens,lookup_ids\n",
    "        \n",
    "    \n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])\n",
    "  def train(self, sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "      #sentence_embeddings = tf.nn.embedding_lookup(self._embeddings,tokens_ids_seq)\n",
    "    \n",
    "      lstm_output = self.model(tokens_ids_seq)\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "      targets = tf.reshape(tokens_ids_target, [-1])\n",
    "      weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "      # Final loss is the mean loss for all token losses.\n",
    "      final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_loss\")\n",
    "\n",
    "    watched = t.watched_variables()\n",
    "    gradients = t.gradient(final_loss, watched)\n",
    "    self.optimizer.apply_gradients(zip(gradients, watched))\n",
    "\n",
    "    #for w, g in zip(watched, gradients):\n",
    "    #  w.assign_sub(g)\n",
    "\n",
    "    return final_loss\n",
    "  \n",
    "  @tf.function(input_signature=[tf.TensorSpec([None], tf.dtypes.string)])  \n",
    "  def validate(self,sentences):\n",
    "    tokens,lookup_ids = self._tokens_to_lookup_ids(sentences)\n",
    "    # Targets are the next word for each word of the sentence.\n",
    "    tokens_ids_seq = lookup_ids[:, 0:-1]\n",
    "    tokens_ids_target = lookup_ids[:, 1:]\n",
    "    tokens_prefix = tokens[:, 0:-1]\n",
    "\n",
    "    # Mask determining which positions we care about for a loss: all positions\n",
    "    # that have a valid non-terminal token.\n",
    "    mask = tf.logical_and(\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"\")),\n",
    "        tf.logical_not(tf.equal(tokens_prefix, \"<E>\")))\n",
    "\n",
    "    input_mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "    lstm_output = self.model(tokens_ids_seq)\n",
    "    lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "    logits = self._logit_layer(lstm_output)\n",
    "      \n",
    "\n",
    "    targets = tf.reshape(tokens_ids_target, [-1])\n",
    "    weights = tf.cast(tf.reshape(input_mask, [-1]), tf.float32)\n",
    "\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=targets, logits=logits)\n",
    "\n",
    "    # Final loss is the mean loss for all token losses.\n",
    "    final_loss = tf.math.divide(\n",
    "          tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "          tf.reduce_sum(weights),\n",
    "          name=\"final_validation_loss\")\n",
    "\n",
    "    return final_loss\n",
    "    \n",
    "  @tf.function\n",
    "  def decode_greedy(self, sequence_length, first_word):\n",
    "\n",
    "    sequence = [first_word]\n",
    "    current_word = first_word\n",
    "    current_id = tf.expand_dims(self._words_to_indices(current_word), 0)\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "      lstm_output = self.model(tf.expand_dims(current_id,0))\n",
    "      lstm_output = tf.reshape(lstm_output, [-1,self._state_size])\n",
    "      logits = self._logit_layer(lstm_output)\n",
    "      softmax = tf.nn.softmax(logits)\n",
    "\n",
    "      next_ids = tf.math.argmax(softmax, axis=1)\n",
    "      next_words = self._indices_to_words(next_ids)[0]\n",
    "      \n",
    "      current_id = next_ids\n",
    "      current_word = next_words\n",
    "      sequence.append(current_word)\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module = ULMFiTModule(vocab=train_vocab,emb_dim=128,buckets=1,state_size=128,n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(2):\n",
    "#    train_loss = module.train(tf.constant(wiki_articles))\n",
    "#    validation_loss = module.validate(tf.constant(wiki_valid_articles))\n",
    "#    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy(),\" Validation loss \",validation_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 10:07:01.917144 139776025814784 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/lookup_ops.py:985: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train loss:  2.7724257  Validation loss  2.769445\n",
      "Epoch  1  Train loss:  2.7691047  Validation loss  2.7662005\n",
      "Epoch  2  Train loss:  2.7657092  Validation loss  2.7627726\n",
      "Epoch  3  Train loss:  2.7621527  Validation loss  2.75908\n",
      "Epoch  4  Train loss:  2.7583556  Validation loss  2.75504\n",
      "Epoch  5  Train loss:  2.754243  Validation loss  2.75057\n",
      "Epoch  6  Train loss:  2.7497408  Validation loss  2.7455854\n",
      "Epoch  7  Train loss:  2.7447755  Validation loss  2.7399938\n",
      "Epoch  8  Train loss:  2.739268  Validation loss  2.733692\n",
      "Epoch  9  Train loss:  2.7331305  Validation loss  2.726563\n",
      "Epoch  10  Train loss:  2.7262645  Validation loss  2.7184708\n",
      "Epoch  11  Train loss:  2.7185562  Validation loss  2.709254\n",
      "Epoch  12  Train loss:  2.7098737  Validation loss  2.6987262\n",
      "Epoch  13  Train loss:  2.7000654  Validation loss  2.6866689\n",
      "Epoch  14  Train loss:  2.6889555  Validation loss  2.6728287\n",
      "Epoch  15  Train loss:  2.6763406  Validation loss  2.6569138\n",
      "Epoch  16  Train loss:  2.6619887  Validation loss  2.6385915\n",
      "Epoch  17  Train loss:  2.6456358  Validation loss  2.6174936\n",
      "Epoch  18  Train loss:  2.626989  Validation loss  2.5932305\n",
      "Epoch  19  Train loss:  2.6057358  Validation loss  2.5654252\n",
      "Epoch  20  Train loss:  2.5815694  Validation loss  2.5337873\n",
      "Epoch  21  Train loss:  2.5542357  Validation loss  2.4982505\n",
      "Epoch  22  Train loss:  2.5236292  Validation loss  2.4592133\n",
      "Epoch  23  Train loss:  2.4899588  Validation loss  2.417904\n",
      "Epoch  24  Train loss:  2.4540014  Validation loss  2.3767955\n",
      "Epoch  25  Train loss:  2.4173975  Validation loss  2.3397202\n",
      "Epoch  26  Train loss:  2.382758  Validation loss  2.311108\n",
      "Epoch  27  Train loss:  2.3531892  Validation loss  2.2943432\n",
      "Epoch  28  Train loss:  2.3311992  Validation loss  2.2901845\n",
      "Epoch  29  Train loss:  2.3176727  Validation loss  2.2960753\n",
      "Epoch  30  Train loss:  2.3114343  Validation loss  2.3066642\n",
      "Epoch  31  Train loss:  2.3094788  Validation loss  2.3159661\n",
      "Epoch  32  Train loss:  2.3082063  Validation loss  2.3197427\n",
      "Epoch  33  Train loss:  2.3049092  Validation loss  2.3162482\n",
      "Epoch  34  Train loss:  2.2983406  Validation loss  2.3057234\n",
      "Epoch  35  Train loss:  2.2884803  Validation loss  2.289611\n",
      "Epoch  36  Train loss:  2.2760832  Validation loss  2.2699158\n",
      "Epoch  37  Train loss:  2.2622802  Validation loss  2.2487657\n",
      "Epoch  38  Train loss:  2.248292  Validation loss  2.228107\n",
      "Epoch  39  Train loss:  2.235224  Validation loss  2.2094634\n",
      "Epoch  40  Train loss:  2.2239025  Validation loss  2.1937454\n",
      "Epoch  41  Train loss:  2.2147453  Validation loss  2.1811335\n",
      "Epoch  42  Train loss:  2.2076852  Validation loss  2.1711352\n",
      "Epoch  43  Train loss:  2.2022169  Validation loss  2.162861\n",
      "Epoch  44  Train loss:  2.197591  Validation loss  2.1553903\n",
      "Epoch  45  Train loss:  2.1930704  Validation loss  2.1480238\n",
      "Epoch  46  Train loss:  2.1881075  Validation loss  2.1403592\n",
      "Epoch  47  Train loss:  2.1823912  Validation loss  2.1322691\n",
      "Epoch  48  Train loss:  2.1758323  Validation loss  2.123877\n",
      "Epoch  49  Train loss:  2.1685455  Validation loss  2.1155345\n",
      "Epoch  50  Train loss:  2.1608336  Validation loss  2.1077433\n",
      "Epoch  51  Train loss:  2.1531327  Validation loss  2.1009939\n",
      "Epoch  52  Train loss:  2.1459005  Validation loss  2.0955505\n",
      "Epoch  53  Train loss:  2.1394632  Validation loss  2.0912864\n",
      "Epoch  54  Train loss:  2.1338966  Validation loss  2.0876663\n",
      "Epoch  55  Train loss:  2.1289961  Validation loss  2.0839055\n",
      "Epoch  56  Train loss:  2.1243682  Validation loss  2.0792284\n",
      "Epoch  57  Train loss:  2.119589  Validation loss  2.0731132\n",
      "Epoch  58  Train loss:  2.1143599  Validation loss  2.0654204\n",
      "Epoch  59  Train loss:  2.1085906  Validation loss  2.056392\n",
      "Epoch  60  Train loss:  2.102402  Validation loss  2.0465407\n",
      "Epoch  61  Train loss:  2.0960562  Validation loss  2.0364885\n",
      "Epoch  62  Train loss:  2.089853  Validation loss  2.0267956\n",
      "Epoch  63  Train loss:  2.0840232  Validation loss  2.0178294\n",
      "Epoch  64  Train loss:  2.0786493  Validation loss  2.0096993\n",
      "Epoch  65  Train loss:  2.0736375  Validation loss  2.0022902\n",
      "Epoch  66  Train loss:  2.068757  Validation loss  1.9953728\n",
      "Epoch  67  Train loss:  2.0637338  Validation loss  1.9887469\n",
      "Epoch  68  Train loss:  2.0583618  Validation loss  1.9823396\n",
      "Epoch  69  Train loss:  2.0525782  Validation loss  1.9762173\n",
      "Epoch  70  Train loss:  2.046469  Validation loss  1.9705105\n",
      "Epoch  71  Train loss:  2.040211  Validation loss  1.9653004\n",
      "Epoch  72  Train loss:  2.0339818  Validation loss  1.9605312\n",
      "Epoch  73  Train loss:  2.0278847  Validation loss  1.955986\n",
      "Epoch  74  Train loss:  2.0219145  Validation loss  1.9513403\n",
      "Epoch  75  Train loss:  2.015975  Validation loss  1.9462627\n",
      "Epoch  76  Train loss:  2.0099323  Validation loss  1.9405216\n",
      "Epoch  77  Train loss:  2.0036764  Validation loss  1.9340489\n",
      "Epoch  78  Train loss:  1.9971596  Validation loss  1.926946\n",
      "Epoch  79  Train loss:  1.990405  Validation loss  1.9194322\n",
      "Epoch  80  Train loss:  1.9834787  Validation loss  1.9117645\n",
      "Epoch  81  Train loss:  1.976448  Validation loss  1.9041609\n",
      "Epoch  82  Train loss:  1.9693439  Validation loss  1.8967533\n",
      "Epoch  83  Train loss:  1.962146  Validation loss  1.8895876\n",
      "Epoch  84  Train loss:  1.9547997  Validation loss  1.8826538\n",
      "Epoch  85  Train loss:  1.9472485  Validation loss  1.8759202\n",
      "Epoch  86  Train loss:  1.9394677  Validation loss  1.8693417\n",
      "Epoch  87  Train loss:  1.9314681  Validation loss  1.8628408\n",
      "Epoch  88  Train loss:  1.9232757  Validation loss  1.8562849\n",
      "Epoch  89  Train loss:  1.9149009  Validation loss  1.8494948\n",
      "Epoch  90  Train loss:  1.9063249  Validation loss  1.842287\n",
      "Epoch  91  Train loss:  1.8975087  Validation loss  1.8345383\n",
      "Epoch  92  Train loss:  1.8884215  Validation loss  1.8262298\n",
      "Epoch  93  Train loss:  1.8790584  Validation loss  1.8174465\n",
      "Epoch  94  Train loss:  1.8694402  Validation loss  1.8083245\n",
      "Epoch  95  Train loss:  1.8595848  Validation loss  1.7989782\n",
      "Epoch  96  Train loss:  1.8494754  Validation loss  1.7894539\n",
      "Epoch  97  Train loss:  1.8390565  Validation loss  1.7797376\n",
      "Epoch  98  Train loss:  1.8282689  Validation loss  1.769788\n",
      "Epoch  99  Train loss:  1.817093  Validation loss  1.7595397\n",
      "Epoch  100  Train loss:  1.8055503  Validation loss  1.7488728\n",
      "Epoch  101  Train loss:  1.7936589  Validation loss  1.7376176\n",
      "Epoch  102  Train loss:  1.7813996  Validation loss  1.72563\n",
      "Epoch  103  Train loss:  1.768729  Validation loss  1.7128816\n",
      "Epoch  104  Train loss:  1.7556171  Validation loss  1.6994785\n",
      "Epoch  105  Train loss:  1.7420604  Validation loss  1.6855881\n",
      "Epoch  106  Train loss:  1.7280532  Validation loss  1.6713334\n",
      "Epoch  107  Train loss:  1.7135613  Validation loss  1.6567515\n",
      "Epoch  108  Train loss:  1.6985466  Validation loss  1.6418024\n",
      "Epoch  109  Train loss:  1.6830108  Validation loss  1.6263669\n",
      "Epoch  110  Train loss:  1.666986  Validation loss  1.6102401\n",
      "Epoch  111  Train loss:  1.6504787  Validation loss  1.593228\n",
      "Epoch  112  Train loss:  1.6334648  Validation loss  1.5753026\n",
      "Epoch  113  Train loss:  1.615942  Validation loss  1.5566411\n",
      "Epoch  114  Train loss:  1.59795  Validation loss  1.5374885\n",
      "Epoch  115  Train loss:  1.5795182  Validation loss  1.5179921\n",
      "Epoch  116  Train loss:  1.5606384  Validation loss  1.4981704\n",
      "Epoch  117  Train loss:  1.5413204  Validation loss  1.4779383\n",
      "Epoch  118  Train loss:  1.5216217  Validation loss  1.4571427\n",
      "Epoch  119  Train loss:  1.5015931  Validation loss  1.4357082\n",
      "Epoch  120  Train loss:  1.4812703  Validation loss  1.4137807\n",
      "Epoch  121  Train loss:  1.4607152  Validation loss  1.391634\n",
      "Epoch  122  Train loss:  1.4399848  Validation loss  1.3694651\n",
      "Epoch  123  Train loss:  1.4191003  Validation loss  1.3473293\n",
      "Epoch  124  Train loss:  1.3981104  Validation loss  1.3251417\n",
      "Epoch  125  Train loss:  1.377093  Validation loss  1.3027422\n",
      "Epoch  126  Train loss:  1.3560867  Validation loss  1.2800907\n",
      "Epoch  127  Train loss:  1.3351096  Validation loss  1.2573348\n",
      "Epoch  128  Train loss:  1.3141817  Validation loss  1.2346674\n",
      "Epoch  129  Train loss:  1.2933103  Validation loss  1.2122021\n",
      "Epoch  130  Train loss:  1.2725186  Validation loss  1.18993\n",
      "Epoch  131  Train loss:  1.2518471  Validation loss  1.1678013\n",
      "Epoch  132  Train loss:  1.2313219  Validation loss  1.145891\n",
      "Epoch  133  Train loss:  1.210969  Validation loss  1.1244092\n",
      "Epoch  134  Train loss:  1.1908158  Validation loss  1.1035284\n",
      "Epoch  135  Train loss:  1.1708742  Validation loss  1.0832462\n",
      "Epoch  136  Train loss:  1.1511614  Validation loss  1.063391\n",
      "Epoch  137  Train loss:  1.1316991  Validation loss  1.0437729\n",
      "Epoch  138  Train loss:  1.1124941  Validation loss  1.024367\n",
      "Epoch  139  Train loss:  1.0935489  Validation loss  1.005307\n",
      "Epoch  140  Train loss:  1.0748614  Validation loss  0.9867148\n",
      "Epoch  141  Train loss:  1.0564212  Validation loss  0.9685604\n",
      "Epoch  142  Train loss:  1.0382255  Validation loss  0.9506852\n",
      "Epoch  143  Train loss:  1.020274  Validation loss  0.93297803\n",
      "Epoch  144  Train loss:  1.00256  Validation loss  0.91551554\n",
      "Epoch  145  Train loss:  0.9850783  Validation loss  0.8984614\n",
      "Epoch  146  Train loss:  0.96781886  Validation loss  0.88184464\n",
      "Epoch  147  Train loss:  0.9507753  Validation loss  0.8654945\n",
      "Epoch  148  Train loss:  0.93395007  Validation loss  0.84925264\n",
      "Epoch  149  Train loss:  0.9173453  Validation loss  0.83320695\n",
      "Epoch  150  Train loss:  0.9009689  Validation loss  0.81757176\n",
      "Epoch  151  Train loss:  0.884831  Validation loss  0.8023727\n",
      "Epoch  152  Train loss:  0.86895025  Validation loss  0.7874391\n",
      "Epoch  153  Train loss:  0.85335124  Validation loss  0.7727339\n",
      "Epoch  154  Train loss:  0.83805776  Validation loss  0.75843877\n",
      "Epoch  155  Train loss:  0.8230935  Validation loss  0.74462885\n",
      "Epoch  156  Train loss:  0.8084806  Validation loss  0.7311669\n",
      "Epoch  157  Train loss:  0.79424417  Validation loss  0.7179846\n",
      "Epoch  158  Train loss:  0.7804032  Validation loss  0.7051878\n",
      "Epoch  159  Train loss:  0.76697135  Validation loss  0.69282943\n",
      "Epoch  160  Train loss:  0.7539541  Validation loss  0.68083566\n",
      "Epoch  161  Train loss:  0.741354  Validation loss  0.6691576\n",
      "Epoch  162  Train loss:  0.7291655  Validation loss  0.65783095\n",
      "Epoch  163  Train loss:  0.71737784  Validation loss  0.64687955\n",
      "Epoch  164  Train loss:  0.7059751  Validation loss  0.6362713\n",
      "Epoch  165  Train loss:  0.6949408  Validation loss  0.6259665\n",
      "Epoch  166  Train loss:  0.6842574  Validation loss  0.6159576\n",
      "Epoch  167  Train loss:  0.67390674  Validation loss  0.6062513\n",
      "Epoch  168  Train loss:  0.6638715  Validation loss  0.5968411\n",
      "Epoch  169  Train loss:  0.65413564  Validation loss  0.58770937\n",
      "Epoch  170  Train loss:  0.6446854  Validation loss  0.57884115\n",
      "Epoch  171  Train loss:  0.63550854  Validation loss  0.57023233\n",
      "Epoch  172  Train loss:  0.62659425  Validation loss  0.56188333\n",
      "Epoch  173  Train loss:  0.6179332  Validation loss  0.55379164\n",
      "Epoch  174  Train loss:  0.6095176  Validation loss  0.5459505\n",
      "Epoch  175  Train loss:  0.6013411  Validation loss  0.5383513\n",
      "Epoch  176  Train loss:  0.5933981  Validation loss  0.5309892\n",
      "Epoch  177  Train loss:  0.58568376  Validation loss  0.5238615\n",
      "Epoch  178  Train loss:  0.5781937  Validation loss  0.51696527\n",
      "Epoch  179  Train loss:  0.57092404  Validation loss  0.5102951\n",
      "Epoch  180  Train loss:  0.56387097  Validation loss  0.5038432\n",
      "Epoch  181  Train loss:  0.5570308  Validation loss  0.49760097\n",
      "Epoch  182  Train loss:  0.5503999  Validation loss  0.49156144\n",
      "Epoch  183  Train loss:  0.54397476  Validation loss  0.4857184\n",
      "Epoch  184  Train loss:  0.53775156  Validation loss  0.4800658\n",
      "Epoch  185  Train loss:  0.53172654  Validation loss  0.4745964\n",
      "Epoch  186  Train loss:  0.525896  Validation loss  0.46930224\n",
      "Epoch  187  Train loss:  0.5202561  Validation loss  0.46417582\n",
      "Epoch  188  Train loss:  0.5148033  Validation loss  0.45921105\n",
      "Epoch  189  Train loss:  0.5095334  Validation loss  0.45440352\n",
      "Epoch  190  Train loss:  0.5044428  Validation loss  0.4497495\n",
      "Epoch  191  Train loss:  0.49952748  Validation loss  0.44524541\n",
      "Epoch  192  Train loss:  0.4947835  Validation loss  0.44088823\n",
      "Epoch  193  Train loss:  0.49020708  Validation loss  0.43667474\n",
      "Epoch  194  Train loss:  0.48579425  Validation loss  0.43260273\n",
      "Epoch  195  Train loss:  0.48154104  Validation loss  0.4286711\n",
      "Epoch  196  Train loss:  0.47744358  Validation loss  0.424879\n",
      "Epoch  197  Train loss:  0.47349784  Validation loss  0.42122558\n",
      "Epoch  198  Train loss:  0.4696999  Validation loss  0.41770905\n",
      "Epoch  199  Train loss:  0.4660459  Validation loss  0.4143273\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\",\n",
    "             \"<S> hello world <E>\", \"<S> who are you? <E>\"]\n",
    "validation_sentences = [\"<S> hello there <E>\", \"<S> how are you doing today <E>\",\"<S> I am fine thank you <E>\"]\n",
    "vocab = [\n",
    "      \"<S>\", \"<E>\", \"hello\", \"there\", \"how\", \"are\", \"you\", \"doing\", \"today\",\"I\",\"am\",\"fine\",\"thank\",\"world\",\"who\"]\n",
    "\n",
    "module = ULMFiTModule(vocab=vocab, emb_dim=10, buckets=1, state_size=128,n_layers=1)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = module.train(tf.constant(sentences))\n",
    "    validation_loss = module.validate(tf.constant(validation_sentences))\n",
    "    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy(),\" Validation loss \",validation_loss.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<S> you', b'hello', b'<E>', b'hello', b'<E>', b'hello', b'<E>', b'hello', b'<E>', b'hello', b'<E>']\n"
     ]
    }
   ],
   "source": [
    " # We have to call this function explicitly if we want it exported, because it\n",
    "  # has no input_signature in the @tf.function decorator.\n",
    "decoded = module.decode_greedy(sequence_length=10, first_word=tf.constant(\"<S> you\"))\n",
    "_ = [d.numpy() for d in decoded]\n",
    "print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(module,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = tf.saved_model.load(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune on IMDB dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    train_loss = module.train(tf.constant(imdb_reviews))\n",
    "    #validation_loss = module.validate(tf.constant(validation_sentences))\n",
    "    print(\"Epoch \",epoch,\" Train loss: \",train_loss.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Head \n",
    "\n",
    "\n",
    "Classifier head takes in the final layer output of the languaage model and first gets the average pool and max pool of the \n",
    "final layer outputs, then passes the concatanation of last time steps hidden state, max pool results and average pool results through given number Dense-dropout-batchnormalization blocks. Finally it produces the classifier output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageClassifier(Model):\n",
    "    def __init__(self,language_module,num_labels,dense_units=(128,128),dropouts=(0.1,0.1)):\n",
    "        \n",
    "        # initialization stuff\n",
    "        super(LanguageClassifier,self).__init__()\n",
    "        self._language_module = language_module\n",
    "        self.model_encoder = language_module.model\n",
    "        \n",
    "        \n",
    "        # classifier head layers\n",
    "        self.dense_layers = [Dense(units,activation=\"relu\") for units in dense_units]\n",
    "        self.dropout_layers = [Dropout(p) for p in dropouts]\n",
    "        self.max_pool_layer = GlobalMaxPooling1D()\n",
    "        self.average_pool_layer = GlobalAveragePooling1D()\n",
    "        self.batchnorm_layer = BatchNormalization()\n",
    "        self.n_layers = len(self.dense_layers)\n",
    "        self.final_layer = Dense(num_labels,activation=\"sigmoid\")\n",
    "        \n",
    "    def __call__(self,sentences):\n",
    "        \n",
    "        tokens,lookup_ids = self._language_module._tokens_to_lookup_ids(sentences)\n",
    "        self.enc_out = self.model_encoder(lookup_ids)\n",
    "        last_h = self.enc_out[:,-1,:]\n",
    "        max_pool_output = self.max_pool_layer(self.enc_out)\n",
    "        average_pool_output = self.average_pool_layer(self.enc_out)\n",
    "        \n",
    "        output = concatenate([last_h,max_pool_output,average_pool_output])\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = self.dense_layers[i](output)\n",
    "            output = self.dropout_layers[i](output)\n",
    "            output = self.batchnorm_layer(output)\n",
    "        \n",
    "        final_output = self.final_layer(output)\n",
    "        return final_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageClassifier(num_labels=2,language_module=module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7027415633201599, Accuracy: 40.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 2, Loss: 0.6726537942886353, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 3, Loss: 0.6493236422538757, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 4, Loss: 0.6254310011863708, Accuracy: 60.000003814697266, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 5, Loss: 0.6113171577453613, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 6, Loss: 0.5895274877548218, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 7, Loss: 0.5615838766098022, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 8, Loss: 0.5289801955223083, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 9, Loss: 0.5038028955459595, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 10, Loss: 0.47262048721313477, Accuracy: 80.0, Test Loss: 0.0, Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "labels = tf.constant([[1],[0],[1],[0],[0]])\n",
    "\n",
    "train_loss_hist = []\n",
    "train_accuracy_hist = []\n",
    "valid_loss_hist = []\n",
    "valid_accuracy_hist = []\n",
    "\n",
    "def track(tl_score,tl_accuracy,vl_score,vl_accuracy):\n",
    "    train_loss_hist.append(tl_score)\n",
    "    train_accuracy_hist.append(tl_accuracy)\n",
    "    valid_loss_hist.append(vl_score)\n",
    "    valid_accuracy_hist.append(vl_accuracy)\n",
    "\n",
    "@tf.function\n",
    "def train_step(samples, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(samples)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  #print(tape.watched_variables())\n",
    "  watched = tape.watched_variables()\n",
    "  gradients = tape.gradient(loss, watched)\n",
    "  optimizer.apply_gradients(zip(gradients, watched))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def test_step(samples, labels):\n",
    "  predictions = model(samples)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  train_step(sentences, labels)\n",
    "    #test_step(validation_sentences, validation_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "    \n",
    "  track(train_loss.result(),train_accuracy.result()*100,train_loss.result(),test_accuracy.result()*100)\n",
    "  # Reset the metrics for the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UESJu2YyRrgW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLuYPRavnEiO"
   },
   "source": [
    "# Get Data for Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3RFuj9nfp0mf",
    "outputId": "1103e732-de69-4bb2-a074-93708b4bf2e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-17 17:29:35--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.86.149\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.86.149|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 190229076 (181M) [application/zip]\n",
      "Saving to: ‘wikitext-103-v1.zip’\n",
      "\n",
      "wikitext-103-v1.zip 100%[===================>] 181.42M  35.3MB/s    in 5.7s    \n",
      "\n",
      "2019-08-17 17:29:42 (31.8 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Wikitext 103\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "f-2AYGUOisTo",
    "outputId": "ec200dea-effc-49cc-e611-974a2519880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-103-v1.zip\n",
      "   creating: wikitext-103/\n",
      "  inflating: wikitext-103/wiki.test.tokens  \n",
      "  inflating: wikitext-103/wiki.valid.tokens  \n",
      "  inflating: wikitext-103/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!unzip wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN32l2-FnvPp"
   },
   "source": [
    "# Get IMDB Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RrKT86XTnyH4",
    "outputId": "99e7e280-9556-44a9-eb70-623a23642c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-17 17:30:10--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  20.1MB/s    in 7.1s    \n",
      "\n",
      "2019-08-17 17:30:18 (11.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCW1QVlWoSpW"
   },
   "outputs": [],
   "source": [
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RF_R6EpkowoH",
    "outputId": "0a9eec5b-3117-441a-8426-3048e2cd772b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb  aclImdb_v1.tar.gz  sample_data  wikitext-103  wikitext-103-v1.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1i03-og9nHXu"
   },
   "source": [
    "# Preprocess Data Wikitext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2HC_P-edJDQ"
   },
   "outputs": [],
   "source": [
    "train_path = \"wikitext-103/wiki.train.tokens\"\n",
    "valid_path = \"wikitext-103/wiki.valid.tokens\"\n",
    "test_path = \"wikitext-103/wiki.test.tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wohw_aClqtcc"
   },
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ = [^=]* = $', line)) != 0\n",
    "    \n",
    "UNK = \"unk\"\n",
    "def read_wiki(filename):\n",
    "    articles = []\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = ''\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article += line\n",
    "        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n",
    "            current_article = current_article.replace('<unk>', UNK)\n",
    "            articles.append(current_article)\n",
    "            current_article = ''\n",
    "    current_article = current_article.replace('<unk>', UNK)\n",
    "    articles.append(current_article)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaU5qAAgkV6Q"
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  x = x.strip().lower()\n",
    "  \n",
    "  # fix html \n",
    "  re1 = re.compile(r'  +')\n",
    "  x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace(' @,@ ',',').replace('\\\\', ' \\\\ ')\n",
    "  x=re1.sub(' ', html.unescape(x))\n",
    "  \n",
    "  \"Add spaces around / and # in `t`. \\n\" \n",
    "  x=re.sub(r'([/#\\n])', r' \\1 ', x)\n",
    "  \n",
    "  \"Remove multiple spaces in `t`.\"\n",
    "  \n",
    "  x=re.sub(' {2,}', ' ', x)\n",
    "  \n",
    "  return '<S> '+x+' <E>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMJJERvvqH6n"
   },
   "outputs": [],
   "source": [
    "wiki_articles = read_wiki(train_path)\n",
    "wiki_valid_articles = read_wiki(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFjxquiKq9MQ"
   },
   "outputs": [],
   "source": [
    "wiki_articles = [preprocess(article) for article in wiki_articles]\n",
    "wiki_valid_articles = [preprocess(article) for article in wiki_valid_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "mD1ydY5xrSTr",
    "outputId": "b3263b58-63f9-4ede-f3bf-7098e61d04e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> = gambia women 's national football team = \n",
      " \n",
      " the gambia women 's national football team represents the gambia in international football competition . the team , however , has not competed in a match recognised by fifa , the sport 's international governing body , despite that organised women 's football has been played in the country since 1998 . the gambia has two youth teams , an under-17 side that has competed in fifa u-17 women 's world cup qualifiers , and an under-19 side that withdrew from regional qualifiers for an under-19 world cup . the development of a national team faces challenges similar to those across africa , although the national football association has four staff members focusing on women 's football . \n",
      " \n",
      " = = the team = = \n",
      " \n",
      " in 1985 , few countries had women 's national football teams . while the sport gained popularity worldwide in later decades , the gambia 's national team only played its first game in 2007 . that game was not fifa-recognised . as of march 2012 , the team was unranked by fifa , and as of the following month the gambia had not played in a fifa-sanctioned match . the team has not participated in major regional and international tournaments , including the women 's world cup , the 2010 african women 's championship or the 2011 all-africa games . \n",
      " the country did not have a fifa-recognised youth national team until 2012 , when the gambia under-17 women 's team competed in confederation of african football qualifiers for the fifa u-17 world cup , to be held in azerbaijan in september 2012 . the gambia had fielded an under-17 team of 24 players , narrowed from an initial pool of 49 young women . two girls from the sos children ’ s village unk were chosen as a members of the team . the gambia first played sierra leone in a pair of qualifying matches for the tournament . gambia won the first match 3-0 in banjul , the gambia 's capital . the return match was delayed in for 24 hours and played in makeni . the gambia beat sierra leone 4-3 to qualify for the final round . the gambia then beat tunisia 1-0 at home and won 2-1 in tunisia . adama tamba and awa demba scored the gambia 's goals . tunisia 's only goal was a gambian own goal . the win qualified gambia for the 2012 azerbaijan world cup . \n",
      " the gambia also has an under-19 team that was to play in the african women 's u-19 championship in 2002 . the gambia 's first match was against morocco , but the team withdrew from the competition . \n",
      " \n",
      " = = background and development = = \n",
      " \n",
      " the development of women 's football in africa faces several challenges , including limited access to education , poverty amongst women , inequalities and human rights abuses targeting women . funding is another issue impacting the game in africa , where most financial assistance comes from fifa and not national football associations . another challenge is the retention of football players . many women footballers leave the continent to seek greater opportunity in europe or the united states . \n",
      " gambia 's national football association was founded in 1952 , and became affiliated with fifa in 1968 . football is the most popular women 's sport in the country , and was first played in an organized system in 1998 . a national competition was launched in 2007 , the same year fifa started an education course on football for women . competition was active on both the national and scholastic levels by 2009 . there are four staffers dedicated to women 's football in the gambia football association , and representation of women on the board is required by the association 's charter . <E>\n"
     ]
    }
   ],
   "source": [
    "print(wiki_articles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki_articles.pkl', 'wb') as f:\n",
    "    pickle.dump(wiki_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki_valid_articles.pkl', 'wb') as f:\n",
    "    pickle.dump(wiki_valid_articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzMhV3Gxt_AD"
   },
   "source": [
    "### Build Vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NRtun-1rvtj"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(data,max_words,min_freq):\n",
    "    counter = Counter([word for para in data for word in para.split()])\n",
    "    vocab = [word[0] for word in counter.most_common() if word[1] >= min_freq]\n",
    "    return vocab[:max_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrRHh1u9syGw"
   },
   "outputs": [],
   "source": [
    "train_vocab = build_vocab(wiki_articles,max_words=60000,min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RjMiAAx0s28U",
    "outputId": "376600f5-d6d5-415f-ca8a-f8b9834af84d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxvQzU-qtmfV"
   },
   "source": [
    "# Preprocess IMDB Dataset\n",
    "\n",
    "labels :  0 for positive, 1 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKJnO0fXujPh"
   },
   "outputs": [],
   "source": [
    "dir_names = ['pos','neg']\n",
    "\n",
    "imdb_file_paths = []\n",
    "imdb_labels = []\n",
    "for i, dir in enumerate(dir_names):\n",
    "  file_names = [os.path.join(\"aclImdb/train\",dir,name) for name in os.listdir(\"aclImdb/train/\"+dir)]\n",
    "  imdb_file_paths += file_names\n",
    "  imdb_labels += [i]*len(os.listdir(\"aclImdb/train/\"+dir))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOzpboWQxJaT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "permutation = np.random.permutation(len(imdb_file_paths))\n",
    "\n",
    "imdb_file_paths = np.array(imdb_file_paths)[permutation]\n",
    "imdb_labels = np.array(imdb_labels)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R5d3uxQ3xbxQ",
    "outputId": "5fc82546-690c-4b95-b260-3a243983bca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(imdb_file_paths),len(imdb_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojda_BNiyJZB"
   },
   "outputs": [],
   "source": [
    "imdb_reviews = []\n",
    "for file in imdb_file_paths:\n",
    "  with open(file,encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "  data = preprocess(data)\n",
    "  imdb_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Irz7MfdyiP8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> an end of an era was released here in the states in spring 2002 with \"the rookie,\" a disney live action film that seemed to be the \"best for last!!!!!\" it took place right here in texas! actually, the story began in west texas, as evidenced by an area code found on a sign over there. it was abou 0\n"
     ]
    }
   ],
   "source": [
    "print(imdb_reviews[0][:300],imdb_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imdb_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb_reviews, f)\n",
    "    \n",
    "with open('data/imdb_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IMDB Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_valid_file_paths = []\n",
    "imdb_valid_labels = []\n",
    "for i, dir in enumerate(dir_names):\n",
    "  file_names = [os.path.join(\"aclImdb/test\",dir,name) for name in os.listdir(\"aclImdb/test/\"+dir)]\n",
    "  imdb_valid_file_paths += file_names\n",
    "  imdb_valid_labels += [i]*len(os.listdir(\"aclImdb/test/\"+dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "permutation = np.random.permutation(len(imdb_valid_file_paths))\n",
    "\n",
    "imdb_valid_file_paths = np.array(imdb_valid_file_paths)[permutation]\n",
    "imdb_valid_labels = np.array(imdb_valid_labels)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_valid_reviews = []\n",
    "for file in imdb_valid_file_paths:\n",
    "  with open(file,encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "  data = preprocess(data)\n",
    "  imdb_valid_reviews.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> i rank opera as one of the better argento films. plot holes and inconsistencies? sure, but i don't think they impair this film as much as many other reviewers seem to. a lot of elements that are in many of argento's films are kinda \"off-the-wall\", but that's part of the draw of his films... \n",
      " \n",
      "  0\n"
     ]
    }
   ],
   "source": [
    "print(imdb_valid_reviews[0][0:300],imdb_valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/imdb_valid_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb_valid_reviews, f)\n",
    "    \n",
    "with open('data/imdb_valid_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(imdb_valid_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ULMFiT Data Prep.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
